---
title: "Combined practices"
author: "Group 8"
date: "2024-12-09"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages(c("tseries", "urca", "forecast", "nortest", "MASS", "fpp3"))

library(tseries)
library(urca)
library(forecast)
library(readxl)
library(dplyr)
library(ggplot2)
library(nortest)
library(MASS)
library(stats)    
library(fGarch)
library(summarytools)
library(visdat)
library(DT)
library(fpp3)
library(tsibbledata)
library(flextable)
library(fable)
library(here)
library(lmtest)
```

## Practical 1

### Part 1: Financial returns and normality

#### 1.1 Read in the Bitcoin data from le Crypto data.csv. Then, assess the stationarity of the (raw) Bitcoin prices.

```{r}
#Uploading the data set
crypto<-read.csv("Crypto_data.csv")

bitcoin_ts <- ts(crypto$Bitcoin, 
                 start = c(1),       
                 frequency = 1440)



plot(bitcoin_ts, type = "l", main = "Bitcoin Prices", xlab = "Time", col = "#2683C6")

# Plotting the Q-Q plot

qqnorm(bitcoin_ts, main = "Normal Q-Q Plot")
qqline(bitcoin_ts, col = "#2683C6")
```

The plots show that the distribution of the data base is not normal and a clear trend over the years.
However, it is needed to review it in detail.
Let's check it out in more detail

```{r}

# Plotting the cumulative periodogram
cpgram(bitcoin_ts, main = "Cumulative Periodogram")
# Check for stationarity using Augmented Dickey-Fuller test
adf_test <- adf.test(crypto$Bitcoin)
print(adf_test)
if (adf_test$p.value > 0.05) {
  cat("The time series is non-stationary. Differencing will be performed to achieve stationarity.\n") }

```

The cumulative periodogram shows clearly that the data base is not stationary as the observations are outside the confidence interval of white noise stationarity.
This statement is confirmed by the Dickey Fuller Test

```{r}
# Plotting the ACF and PACF
par(mfrow = c(2, 2))
acf(bitcoin_ts, main = "ACF Plot")
pacf(bitcoin_ts, main = "PACF Plot")
par(mfrow = c(1, 1))

```

The no stationarty is confirmed by the PACF and ACF plots as they show clear patterns in both of them.

```{r}
# KPSS Test
kpss_test <- ur.kpss(bitcoin_ts)
summary(kpss_test)
```

Finally, the KPSS test suggests that we can rejec the Ho of stationarity concluding that the prices of the Bitcoin are not stationary.

#### 1.2.Create a function to transform the Bitcoin prices into their negative log returns counterparts. Plot the latter series and assess their stationarity. To compare the series, also plot the negative log returns on a common scale.

```{{r}}
bitcoin_log_returns <- diff(log(bitcoin_ts))
plot(bitcoin_log_returns, type = "l", main = "Negative Log Returns of Bitcoin Prices", 
     ylab = "Negative Log Returns", xlab = "Time", col = "#62A39F")
```

This graph suggest us that the negative logaritmic returns are more similar to a stationary data base than the original one.

```{r}
plot(bitcoin_ts, type = "l", col = "#1CADE4", lwd = 2, 
     main = "Bitcoin Prices and Negative Log Returns", 
     ylab = "Bitcoin Prices", xlab = "Time")

# Add the second series (negative log returns) on a secondary axis
par(new = TRUE)
plot(bitcoin_log_returns, type = "l", col = "#62A39F", lwd = 2, axes = FALSE, 
     xlab = "", ylab = "")
axis(side = 4) # Add axis on the right side
mtext("Negative Log Returns", side = 4, line = 3) # Label for the secondary axis

# Add a legend
legend("topright", legend = c("Bitcoin Prices", "Negative Log Returns"), 
       col = c("#1CADE4", "#62A39F"), lty = 1, lwd = 2)
```

When we plot it together, it is easy to see how the logaritmic data set is closer to what we can call stationarity.

#### 1.3. Are the negative log returns normally distributed? Draw histograms, check QQ-plots and use an Anderson-Darling testing procedure to answer this question.

```{{r}}
# Plot Histogram of negative log returns
hist(bitcoin_log_returns, breaks = 50, main = "Histogram of Negative Log Returns",
     xlab = "Negative Log Returns", col = "#62A39F", border = "black", probability = TRUE)


# QQ-Plot to check for normality
qqnorm(bitcoin_log_returns, main = "QQ-Plot of Negative Log Returns")
qqline(bitcoin_log_returns, col = "#62A39F", lwd = 2)


```

The histogram suggests a normal distributions while the QQplot shows some desviations at the tails that make us believe that there could be some problems with the normality.

```{r}
print(ad.test(bitcoin_log_returns))
```

This test helps us to confirm our observation about the normal distribution.
As the p value is lower than 5% we can reject the normality and confirm that the negative log returns are not normally distributed.

#### 1.4. Fit a t-distribution to the negative log returns using fitdistr(). Using a QQ-plot, decide whether the t is better than with a Normal distribution, based on your answer in (3).

```{{r}}
bitcoin_log_returns_nots <- crypto$Bitcoin[!is.nan(crypto$Bitcoin) & is.finite(crypto$Bitcoin)]


fit_t <- fitdistr(bitcoin_log_returns_nots, "t")

fit_t_dis<- (crypto$Bitcoin-fit_t$estimate[1])/fit_t$sd[1]

fit_t_dis_ts<-ts(fit_t_dis, 
                 start = c(1),       
                 frequency = 1440)

qqnorm(fit_t_dis_ts, main = "QQ-Plot of Negative Log t Returns")
qqline(fit_t_dis_ts, col = "#62A39F", lwd = 2)

```

Comparing this chart with the normal distribution plot, we can say that the normal distribution was better than the t distribution as it is closer to the blue line.

#### 1.5. Compare the tails of the densities of the t-distribution and the normal distribution. Can we expect more extreme, unexpected events in t-distribution or in normal distribution? What can you conclude about the extreme events of our bitcoin data?

```         
Both tails deviate significantly from the blue line, indicating that Bitcoin is prone to extreme events. However, the t-distribution exhibits greater tail density compared to the normal distribution, suggesting that the t-distribution allows for a higher likelihood of unexpected, extreme events.
```

### Part 2: Financial time series, heteroscedasticity and the random walk hypothesis

Another crucial hypothesis in asset pricing is the so-called homoscedasticity, i.e. constant variance of the residuals.
We would also like to check this assumption.
We use the same Bitcoin data as in Part 1.

#### 2.1 Plot the ACF of the raw series as well as the negative log returns. Which one do you think are easier to model?

```{r}
# Plot ACF of the raw Bitcoin series
acf(crypto$Bitcoin, main = "ACF of Raw Bitcoin Prices", lag.max = 50)

# Calculate the negative log returns
log_returns <- diff(log(crypto$Bitcoin))  # Log returns
neg_log_returns <- -log_returns            # Negative log returns

# Plot ACF of the negative log returns
acf(neg_log_returns, main = "ACF of Negative Log Returns", lag.max = 50)
```

The ACF of the raw Bitcoin price series shows high autocorrelation at multiple lags, indicating a strong positive, declining trend and non-stationarity.
This suggests that the raw series exhibits significant dependencies and is harder to model.
In contrast, the ACF of the negative log returns is mostly within the confidence intervals, suggesting little to no autocorrelation and a stationary series.
Since the negative log returns exhibit less persistence and randomness, they are easier to model and more suitable for time series analysis compared to the raw Bitcoin prices.

#### 2.2 Use a Ljung-Box procedure to formally test for (temporal) serial dependence in the raw series and in the negative log return series. What is your conclusion?

```{r}
ljung_box_raw <- Box.test(crypto$Bitcoin, type = "Ljung-Box", lag = 20)
print(ljung_box_raw)


ljung_box_neg_log <- Box.test(neg_log_returns, type = "Ljung-Box", lag = 20)
print(ljung_box_neg_log)
```

Since the p-value is much smaller than 0.05, we reject the null hypothesis, indicating that there is significant autocorrelation in the raw Bitcoin price series.
This confirms that the raw Bitcoin series is non-stationary and exhibits strong trends and patterns over time, as suggested by the ACF plot.
Since the p-value is just below 0.05, we also reject the null hypothesis, suggesting weak serial dependence in the negative log returns, though the level of dependence is much weaker compared to the raw Bitcoin series.
While the negative log returns series is closer to stationarity (as indicated by the ACF plot), there is still a slight level of autocorrelation present.

#### 2.3 Propose ARIMA models for the negative log returns series, based on visualization tools (e.g. ACF, PACF). Select an ARIMA model using auto.arima() (forecast package) for the negative log returns series. Comment on the difference. Assess the residuals of the resulting models.

```{r}
# Fit a manually proposed ARIMA model 
manual_ARIMA_model <- arima(neg_log_returns, order = c(1, 0, 1))

# Print the manually selected model
print(manual_ARIMA_model)


# Use auto.arima() to select the best ARIMA model
auto_ARIMA_model <- auto.arima(neg_log_returns)

# Print the selected ARIMA model
print(auto_ARIMA_model)

# Assess residuals of the manually proposed ARIMA model
checkresiduals(manual_ARIMA_model)

# Assess residuals of the auto.arima() model
checkresiduals(auto_ARIMA_model)
```

The auto ARIMA(2, 0, 2) model is likely preferred because it has a lower AIC value, suggesting a better balance between fit and complexity.
The first model provides a simpler representation but may not capture the underlying dynamics as effectively as the second model.
The stronger coefficients in the second model suggest it can explain the data with greater accuracy.
We reject the null hypothesis of no autocorrelation in the residuals for manual ARIMA(1, 0, 1) model.
This indicates that there is significant autocorrelation.
The model may not adequately capture the time series dynamics, suggesting that it might be beneficial to consider a more complex model.
For auto ARIMA(2, 0, 2) we accept the null hypothesis of no autocorrelation in the residuals.
This indicates that the residuals from the ARIMA(2, 0, 2) model behave like white noise, suggesting that this model adequately captures the time series dynamics.

#### 2.4 Fit GARCH models to the negative log returns with both normal and standardized t-distributions, with order (1, 1), using the garchFit() function from the fGarch library. Assess the quality of the fit by evaluating the residuals.

```{r}
# Fit GARCH(1, 1) with normal distribution
garch_normal <- garchFit(formula = ~ garch(1, 1), data = neg_log_returns, 
                          cond.dist = "norm")

# Print the summary of the GARCH model with normal distribution
summary(garch_normal)

# Fit GARCH(1, 1) with standardized t-distribution
garch_t <- garchFit(formula = ~ garch(1, 1), data = neg_log_returns, 
                     cond.dist = "std")

# Print the summary of the GARCH model with t-distribution
summary(garch_t)

# Assess residuals for GARCH model with normal distribution
residuals_normal <- residuals(garch_normal)

# Evaluate residuals for GARCH model with normal distribution
par(mfrow = c(2, 2))  # Set up plot layout for residual diagnostics

# Plot residuals for the GARCH model with normal distribution
plot(residuals_normal, main = "Residuals of GARCH Model (Normal)", 
     ylab = "Residuals", xlab = "Time")

# ACF of residuals for the GARCH model with normal distribution
acf(residuals_normal, main = "ACF of Residuals (Normal)")

# Assess residuals for GARCH model with t-distribution
residuals_t <- residuals(garch_t)
# Plot residuals for the GARCH model with t-distribution
plot(residuals_t, main = "Residuals of GARCH Model (t-Distribution)", 
     ylab = "Residuals", xlab = "Time")

# ACF of residuals for the GARCH model with t-distribution
acf(residuals_t, main = "ACF of Residuals (t-Distribution)")
```

The GARCH(1, 1) model with standardized t-distribution appears to be the better fitting model for the negative log returns, given the log-likelihood, AIC, and the inclusion of a shape parameter.
Both models exhibit significant coefficients for ( \omega ), ( \alpha\_1 ), and ( \beta\_1 ).
Despite both models showing non-normality in residuals, they both adequately model the autocorrelation structure of the returns.

#### 2.5 Residual serial correlation can be present when fitting a GARCH directly on the negative log returns.

Hence, in order to circumvent this problem, it is possible to use the following two-step approach: • Fit an ARIMA(p, d, q) on the negative log returns with the choices p, d and q from part (c); • Fit a GARCH(1, 1) on the residuals of the ARIMA(p, d, q) fit.
Proceed with the above recipe.
Assess the quality of the above fit.

```{r}
# Obtain the residuals from the ARIMA model
arima_residuals <- residuals(auto_ARIMA_model)

# Fit a GARCH(1, 1) model on the ARIMA residuals
garch_arima <- garchFit(formula = ~ garch(1, 1), data = arima_residuals, cond.dist = "norm")

# Print the summary of the GARCH model fitted on ARIMA residuals
summary(garch_arima)

# Assess the residuals of the GARCH model
# Check for autocorrelation in the residuals
garch_residuals <- residuals(garch_arima)

# Plot ACF of standardized residuals
acf(garch_residuals)

# Plot PACF of standardized residuals
pacf(garch_residuals)
```

The GARCH(1, 1) model on the residuals of the ARIMA(2, 0, 2) model indicates significant volatility persistence, as evidenced by the significant α1 and β1 coefficients.
The Ljung-Box test results suggest that the residuals do not exhibit serial correlation, indicating a good fit for the model.
ACF and PACF of the residuals indicate that GARCH model has adequately captured the temporal dependencies in the data suggesting that the model is a good fit for the data.

#### 2.6 Compare the three models from the previous parts. Which is more suitable? In which of these models is the homoscedasticity assumption violated?

To compare the three models (ARIMA, GARCH, and ARIMA-GARCH) we consider various criteria such as model fit, statistical significance, and the homoscedasticity assumption.
The ARIMA-GARCH model is the most suitable because it effectively captures both the mean and the variance of the negative log returns.
It addresses the limitations of the ARIMA model alone, which may not handle volatility adequately, while also confirming the improvements in fit over the standalone GARCH model.
The ARIMA model is where the homoscedasticity assumption is most likely violated, as it does not account for the changing variance inherent in financial time series data.
In contrast, both the GARCH and ARIMA-GARCH models explicitly model volatility, thereby addressing this assumption.

### Part 3:Dependenlibrary(forecast)ce between time series

We would like to know the connection between the two most prominent nancial crypto-stock prices \| Bitcoin and Ethereum.
Are they dependent?
Are the extreme events between these time series connected?

For this part, use also the Ethereum data (ETH).
Compute the negative log returns of ETH.

```{r}
# Calculate log returns for Bitcoin and Ethereum (convert to numeric in case of any issues)
crypto <- crypto %>%
  mutate(Bitcoin = as.numeric(Bitcoin),
         Ethereum = as.numeric(Ethereum),
         Bitcoin_log_return = log(Bitcoin / lag(Bitcoin)),
         Ethereum_log_return = log(Ethereum / lag(Ethereum)))

# Calculate negative log returns for both Bitcoin and Ethereum
crypto <- crypto %>%
  mutate(Bitcoin_negative_log_return = -Bitcoin_log_return,
         Ethereum_negative_log_return = -Ethereum_log_return)


# Check the results
head(crypto)
```

```{r}
# Plot the log returns of Bitcoin and Ethereum
plot(crypto$Bitcoin_log_return, type = "l", col = "#2683C6", 
     main = "Log Returns of Bitcoin and Ethereum", ylab = "Log Returns")
lines(crypto$Ethereum_log_return, col = "#3E8853")
legend("topright", legend = c("Bitcoin", "Ethereum"), col = c("#2683C6", "#3E8853"), lty = 1)
```

#### 3.1 Are the negative log returns of Bitcoin and ETH dependent? Compute the correlation using cor.test() function. Can we conclude that these series are independent?

```{r}
# Check if the columns are numeric
str(crypto)

# Drop the first row with NA values from the log return calculations (due to lag)
crypto <- na.omit(crypto) 

# Check if there are any NA values remaining
sum(is.na(crypto$Bitcoin_negative_log_return))  # Should return 0
sum(is.na(crypto$Ethereum_negative_log_return))  # Should return 0

# Perform the correlation test between Bitcoin and Ethereum negative log returns
correlation_test <- cor.test(crypto$Bitcoin_negative_log_return, crypto$Ethereum_negative_log_return)

# Print the results of the correlation test
print(correlation_test)

```

1.  Correlation coefficient (cor):

    -   The correlation coefficient is **-0.00315**, which is extremely close to **0**.

    -   This suggests that there is **no significant correlation** between the **negative log returns** of Bitcoin and Ethereum.
        In other words, extreme negative movements in Bitcoin and Ethereum prices do not seem to be linearly related.

2.  p-value:

    -   The **p-value is 0.9**, which is much higher than the typical significance threshold (**0.05**).

    -   This means that there is **insufficient evidence to reject the null hypothesis** that the correlation is zero (independence).
        Therefore, we cannot conclude that the negative returns of Bitcoin and Ethereum are correlated.

3.  Confidence interval:

    -   The 95% confidence interval for the correlation coefficient is **[-0.0548, 0.0485]**, which includes **0**, further confirming the **lack of a significant correlation.**

Conclusion:

Given the extremely low correlation coefficient, the high p-value, and the confidence interval that includes zero, there is no no significant linear relationship.
This implies that negative extreme events in Bitcoin prices are not related to negative extreme events in Ethereum prices.

#### 3.2 Calculate the cross-correlation function (CCF) between the negative log returns of Bitcoin and ETH. What do you observe?

```{r}
# Calculate the cross-correlation function (CCF) between Bitcoin and Ethereum negative log returns
ccf_result <- ccf(crypto$Bitcoin_negative_log_return, crypto$Ethereum_negative_log_return, lag.max = 20, plot = TRUE)

```

X-axis (Lag):

-   The **lag values** on the X-axis range from -20 to +20.

-   A **negative lag** (to the left of 0) suggests that **Bitcoin’s negative log returns** might be leading Ethereum’s negative log returns (i.e., Bitcoin affects Ethereum after a delay).

-   A **positive lag** (to the right of 0) suggests that **Ethereum’s negative log returns** might be leading Bitcoin’s negative log returns (i.e., Ethereum affects Bitcoin after a delay).

Y-axis (ACF - Autocorrelation):

-   The Y-axis shows the strength of the cross-correlation for each lag. Correlation values closer to **1** indicate a strong positive relationship, while values closer to **-1** indicate a strong negative relationship. Values around **0** suggest little to no correlation.

Confidence intervals:

-   The blue dashed lines represent the **confidence intervals** (typically 95%).
    If a cross-correlation value falls outside these bounds, it is considered **statistically significant**, meaning there is likely some dependence between the two series at that specific lag.

-   Values within the dashed lines indicate **no significant correlation** at those lags.

Conclusion:

-   The CCF plot shows **no significant lead-lag relationship** between the negative log returns of Bitcoin and Ethereum.
    The only notable correlation occurs at **lag 0**, implying that the two assets' negative log returns are correlated when they happen **at the same time**, but there is no evidence of either asset consistently leading the other in terms of negative returns.

-   This result is consistent with the Pearson correlation analysis.

#### 3.3 Is one of the time series good predictor of the second? Assess whether there is any predictive power between the negative log returns of Bitcoin and ETH. You can use grangertest() in the lmtest package with carefully chosen hyperparameter order. What is your conclusion?

```{r}
# Fit ARIMA model to Bitcoin negative log returns and auto-select order based on AIC/BIC
fit_btc <- auto.arima(crypto$Bitcoin_negative_log_return, ic = "aic")

# Check the selected ARIMA order (p, d, q)
summary(fit_btc)

# Similarly, for Ethereum
fit_eth <- auto.arima(crypto$Ethereum_negative_log_return, ic = "aic")

# Check the selected ARIMA order for Ethereum
summary(fit_eth)
```

**AR terms (p)** from the ARIMA models:

-   Bitcoin: AR(2)

-   Ethereum: AR(2)

These values as a guideline for choosing the lag order in the **Granger causality test**.

For instance:

-   Use **2 lags** in the Granger test (as both Bitcoin and Ethereum seem to have strong autoregressive effects at lag 2).

-   try testing with higher lag orders (e.g., 4 lags, especially for Ethereum since the MA terms are high) to see if the results differ.

```{r}

# Granger causality test with 2 lags based on ARIMA models
granger_test_btc_to_eth <- grangertest(Ethereum_negative_log_return ~ Bitcoin_negative_log_return, order = 2, data = crypto)
granger_test_eth_to_btc <- grangertest(Bitcoin_negative_log_return ~ Ethereum_negative_log_return, order = 2, data = crypto)

# Print results
print("Granger causality test: Bitcoin causing Ethereum")
print(granger_test_btc_to_eth)

print("Granger causality test: Ethereum causing Bitcoin")
print(granger_test_eth_to_btc)
```

1.  **Bitcoin Causing Ethereum (Granger Test Results)**:

    **Null Hypothesis**: Bitcoin's negative log returns **do not Granger cause** Ethereum's negative log returns.

**p-value (Pr(\>F))**: The p-value is **0.062**, which is slightly above the typical significance level of **0.05**, but below **0.1**.

Thus, **there is weak evidence** (at the 10% level) that **Bitcoin might have some predictive power** over Ethereum, but the evidence is not strong enough to be conclusive at the standard 5% level.

2.  **Ethereum Causing Bitcoin (Granger Test Results)**:

**Null Hypothesis**: Ethereum's negative log returns **do not Granger cause** Bitcoin's negative log returns.

**p-value (Pr(\>F))**: The p-value is **0.53**, which is well above the common significance levels (0.05 or 0.1).

-   Since the p-value is much higher than 0.05, you **fail to reject the null hypothesis**.

-   This suggests that **Ethereum’s negative log returns do not have predictive power** over Bitcoin’s negative log returns.

Conclusion:

1.  **Bitcoin causing Ethereum**: There is **weak evidence** that Bitcoin's negative log returns might have some predictive power over Ethereum's negative log returns, as the p-value is **0.062** (which is slightly below 0.1 but above 0.05).
    At a **10% significance level**, Bitcoin as having **some predictive power** over Ethereum, but this result is not very strong or conclusive.

2.  **Ethereum causing Bitcoin**: There is **no evidence** that Ethereum's negative log returns Granger cause Bitcoin's negative log returns, as the p-value is **0.53**, which is much higher than the standard significance levels of 0.05 and 0.1.

```{r}
# Granger Causality Test with 3 Lags
granger_test_btc_to_eth_lag3 <- grangertest(Ethereum_negative_log_return ~ Bitcoin_negative_log_return, order = 3, data = crypto)
granger_test_eth_to_btc_lag3 <- grangertest(Bitcoin_negative_log_return ~ Ethereum_negative_log_return, order = 3, data = crypto)

# Print results for 3 lags
print("Granger causality test (3 lags): Bitcoin causing Ethereum")
print(granger_test_btc_to_eth_lag3)

print("Granger causality test (3 lags): Ethereum causing Bitcoin")
print(granger_test_eth_to_btc_lag3)

# Granger Causality Test with 4 Lags
granger_test_btc_to_eth_lag4 <- grangertest(Ethereum_negative_log_return ~ Bitcoin_negative_log_return, order = 4, data = crypto)
granger_test_eth_to_btc_lag4 <- grangertest(Bitcoin_negative_log_return ~ Ethereum_negative_log_return, order = 4, data = crypto)

# Print results for 4 lags
print("Granger causality test (4 lags): Bitcoin causing Ethereum")
print(granger_test_btc_to_eth_lag4)

print("Granger causality test (4 lags): Ethereum causing Bitcoin")
print(granger_test_eth_to_btc_lag4)
```

Granger Causality Test Results with Different Lags:

Granger Causality Test (3 Lags):

-   **Bitcoin causing Ethereum**:

    -   **F-statistic = 5.13**, **p-value = 0.0016** (very significant, below 0.01).

    -   Since the p-value is **less than 0.05**, we **reject the null hypothesis** and conclude that Bitcoin’s negative log returns **Granger cause** Ethereum’s negative log returns when using 3 lags.

    -   This means that past values of Bitcoin's returns (up to 3 days) can predict Ethereum's returns with **strong evidence**.

-   **Ethereum causing Bitcoin**:

    -   **F-statistic = 0.46**, **p-value = 0.71** (far greater than 0.05).

    -   Since the p-value is **greater than 0.05**, we **fail to reject the null hypothesis**, meaning that Ethereum's negative log returns do **not** Granger cause Bitcoin's negative log returns with 3 lags.

**Granger Causality Test (4 Lags)**:

-   **Bitcoin causing Ethereum**:

    -   **F-statistic = 7.09**, **p-value = 1.2e-05** (extremely significant, much below 0.001).

    -   With a very low p-value, we can **strongly reject the null hypothesis** and conclude that Bitcoin’s negative log returns **Granger cause** Ethereum’s negative log returns when using 4 lags.

    -   The predictive power of Bitcoin on Ethereum is even **stronger** when using 4 lags compared to 3 lags.

-   **Ethereum causing Bitcoin**:

    -   **F-statistic = 0.54**, **p-value = 0.71** (again, far greater than 0.05).

    -   As with the 3-lag test, we **fail to reject the null hypothesis**, meaning that Ethereum’s negative log returns do **not** Granger cause Bitcoin’s negative log returns with 4 lags.

Conclusion:

-   **Bitcoin's** past returns **have significant predictive** power over Ethereum's returns, and this relationship is stronger with a **4-lag model**.

-   **Ethereum's** returns **do not have predictive power** over Bitcoin’s returns, based on the 2-lag, 3-lag and 4-lag tests.

The evidence here strongly suggests a **unidirectional relationship** where **Bitcoin influences Ethereum**, but **Ethereum does not influence Bitcoin**.

#### 3.4 Based on your answer in (c), answer the following questions:

```         
1.  We observe an extreme sudden drop in Bitcoin stocks. What should we expect that will happen with ETH stocks?
```

-   The Granger causality suggests that **Bitcoin’s past behavior influences Ethereum’s future performance**, so a significant negative movement in Bitcoin (such as a sudden drop) could lead to a **similar negative impact on Ethereum** over the next few days (based on the lag structure).

-   The effect might not be immediate, but it could unfold over the subsequent few days, particularly within **3 to 4 days** after the drop in Bitcoin.

2)  We observe an extreme sudden drop in ETH stocks. What should we expect that will happen with Bitcoin stocks?

-   According to the Granger causality test, if there is an extreme sudden drop in Ethereum stocks, **we should not expect Bitcoin stocks to be directly influenced** by this drop, at least not in a predictable or systematic way based on the historical relationship between the two assets.

## Practical 2: Precipitation in Lausanne

### Part 1: Block maxima approach

```{r setup, include=FALSE}
library(here)
library(readxl)
library(readr)
library(ggplot2)
library(stringr)
library(fitdistrplus)
library(dplyr)
library(extRemes)
library(evd)
library(ismev)
```

```{r, include=TRUE}
rain_df <- read_csv("Practical 2/Precipitation_lausanne_full.csv")
rain_df$Date <- as.Date(rain_df$Date, format = "%m/%d/%Y")
```

#### 1.1 Histogram of the daily precipitation values:

```{r, include=TRUE}
daily_precipitation_histogram <- ggplot(rain_df, aes(x = Precipitation)) +
  geom_histogram(binwidth = 5, fill = "#3E8853", color = "#3E8853", alpha = 0.7) +
  labs(
    title = "Histogram of Daily Precipitation in Lausanne",
    x = "Daily Precipitation (mm)",
    y = "Frequency"
  ) +
  theme_minimal()

ggsave("daily_precipitation_histogram.png", plot = daily_precipitation_histogram, width = 8, height = 6)

```

According to the histogram, it appears that the data could fit into a Gumbel Distribution.
There is a rapid decay in the frequency of observations and the Gumbel distribution can model this behavior well and also considering that we are focusing on the extreme heavy rainfall events.

#### 1.2 Yearly maximum values:

```{r setup, include=TRUE}
rain_df <- rain_df |> mutate(Year = format(Date, "%Y"))

all_years <- data.frame(Year = as.character(1930:2014))

yearly_max <- rain_df %>%
  group_by(Year) %>%
  summarize(MaxPrecipitation = max(Precipitation, na.rm = TRUE))

histogram_plot <- ggplot(yearly_max, aes(x = MaxPrecipitation)) +
  geom_histogram(binwidth = 5, fill = "#1CADE4", color = "#1CADE4", alpha = 0.7) +
  labs(
    title = "Histogram of Yearly Maximum Precipitation in Lausanne (1930 - 2014)",
    x = "Maximum Precipitation (mm)",
    y = "Frequency"
  ) +
  theme_minimal()

ggsave("yearly_max_precipitation_histogram.png", plot = histogram_plot, width = 8, height = 6)

```

The majority of the yearly maximum precipitation values are clustered between 40 and 70 mm, indicating that these values are typical for Lausanne's climate during the period analyzed.
Precipitation levels exceeding 100 mm are rare, as seen by the few bars on the far right of the histogram.

```{r setup, include=TRUE}
gumbel <- fgev(yearly_max$MaxPrecipitation, shape = 0)
frechet <- fgev(yearly_max$MaxPrecipitation)
```

By examining the scale parameters of the distributions, which indicate the variability of the data, we can observe that the Frechet distribution has the smallest scale parameter (9.97), closely followed by the Gumbel distribution (10.30), both indicating less variation in the extreme precipitation events.
Both the Gumbel and Frechet distributions show a typical yearly maximum precipitation around 48 mm, which is consistent with what was observed in the histogram.
Although both distributions have similar deviance values, the Frechet distribution shows the lowest deviance (666.94), compared to the Gumbel distribution (668.33), suggesting that Frechet provides the best fit for the data.
However, the Gumbel distribution also performs reasonably well, and while the Frechet is the better fit, Gumbel remains a valid option for modeling the yearly maximum precipitation events.

#### 1.3 Linear model to the yearly maximum precipitation values and prediction for the next 10 years:

```{r setup, include=TRUE}
yearly_max$Year <- as.numeric(yearly_max$Year)

linear_model <- lm(MaxPrecipitation ~ Year, data = yearly_max)
summary(linear_model)
```

According to the coefficients, the intercept is not particularly useful in this context since we are not interested in knowing the precipitation when the year is 0, which is not meaningful.
The coefficient for Year -.006, indicating that the maximum precipitation is expected to decrease by approximately .006 mm per year.
However, given that the p-value is higher than the standard significance level (0.05), we cannot conclude that Year is a significant predictor of maximum precipitation.
This is further supported by the relatively low t-value for Year, suggesting weak evidence for its effect.
Additionally, the model has low explanatory power, as reflected by the R-squared, meaning that the model explains only a small portion of the variation in the data.
This suggests that the linear model with Year as the regressor may not be appropriate for capturing the trends in yearly maximum precipitation.

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
any(is.na(yearly_max$Year))
yearly_max <- yearly_max[!is.na(yearly_max$Year), ]
future_years <- data.frame(Year = seq(max(yearly_max$Year) + 1, max(yearly_max$Year) + 10))

predictions <- predict(linear_model, newdata = future_years, interval = "confidence")

future_predictions <- cbind(future_years, predictions)

precipitation_plot <- ggplot() +
  geom_point(data = yearly_max, aes(x = as.numeric(Year), y = MaxPrecipitation), color = "#1CADE4") +
  geom_smooth(data = yearly_max, aes(x = as.numeric(Year), y = MaxPrecipitation), method = "lm", color = "#27CED7", se = FALSE) +
  geom_line(data = future_predictions, aes(x = Year, y = fit), color = "#42BA97") +
  geom_ribbon(data = future_predictions, aes(x = Year, ymin = lwr, ymax = upr), alpha = 0.2, fill = "#42BA97") +
  labs(
    title = "Linear Model for Yearly Maximum Precipitation with Predictions",
    x = "Year",
    y = "Maximum Precipitation (mm)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )

ggsave("linear_model_precipitation_predictions.png", plot = precipitation_plot, width = 10, height = 6)
```

This approach is clearly not reasonable.
As stated before, the linear model is not accurate because the predictor, Year, is not statistically significant, and the model has a low explanatory power (as indicated by the low R-squared value).
It is not appropriate to assume that maximum precipitation levels will increase each year simply because time has passed.
Precipitation levels are influenced by numerous other significant factors (such as climate patterns, atmospheric conditions, and environmental changes) that are not accounted for in this simple model.
Therefore, relying on Year alone as a predictor of future precipitation is misleading and oversimplified.

#### 1.4 GEV with constant parameters vs. GEV model with time varying location parameter

```{r setup, include=TRUE}
gev_constant <- fevd(yearly_max$MaxPrecipitation, type = "GEV")
summary(gev_constant)

gev_time_var <- fevd(yearly_max$MaxPrecipitation, location.fun = ~ Year, data = yearly_max, type = "GEV")
summary(gev_time_var)
```

The constant-parameter model has a lower AIC/BIC than the time-varying location model, suggesting that the average of extreme precipitation values is changing over time (not simply increasing), as the constant model might imply, but also decreasing at times, which is more aligned with real-world variability.
This highlights that extremes are not static and can fluctuate due to multiple factors beyond just the passage of time, making the time-varying model a more realistic representation.

#### 1.5 Diagnostic plots of the GEV fit

```{r setup, include=TRUE}
fitted_values <- fitted(gev_constant)

png("probability_plot.png", width = 800, height = 600)
plot(gev_constant, type = "probprob", main = "Probability Plot") 
dev.off()  


png("qq_plot.png", width = 800, height = 600) 
plot(gev_constant, type = "qq", main = "Q-Q Plot")
dev.off() 
```

Using the probability plot, which compares the empirical probabilities from the data with the model's predicted probabilities, the points generally align along the diagonal.
However, there are some deviations at the extremes, indicating that the model struggles to fit the most extreme values.
This issue is also observed in the Q-Q plot, which further suggests that the model is not accurate for predicting extreme events.
While the GEV model provides a better fit compared to the previous model, it still falls short in reliably predicting the most extreme precipitation events, particularly at the tails of the distribution.

#### 1.6 Predict the 10-year return level:

```{r setup, include=TRUE}

return_level_10_year <- return.level(gev_constant, return.period = 10)

return_level_df <- data.frame(
    Year = yearly_max$Year,
    MaxPrecipitation = yearly_max$MaxPrecipitation,
    Predicted_10yr_Return_Level = rep(return_level_10_year, length(yearly_max$Year))  
)

plot <- ggplot() +
  geom_point(data = return_level_df, aes(x = Year, y = MaxPrecipitation), color = "#27CED7", size = 3) +
  geom_line(data = return_level_df, aes(x = Year, y = Predicted_10yr_Return_Level), color = "#62A39F", linewidth = 1.2) +
  labs(
    title = "10-Year Return Level Predictions vs Historical Yearly Max Precipitation",
    x = "Year",
    y = "Precipitation (mm)"
  ) +
  scale_x_continuous(breaks = seq(min(return_level_df$Year), max(return_level_df$Year), by = 5)) + 
  theme_minimal()

print(plot)
ggsave("plot2_6.png", plot = plot, width = 7, height = 7)

```

The blue dots represent the observed maximum precipitation for each year, while the green line represents the 10-year return level predicted by the constant-parameter GEV model.
The horizontal nature of the red line reflects the assumption of a constant location parameter, meaning the threshold for extreme events does not change over time.
While the observed blue points vary from year to year, there is no evidence of an increasing trend in extreme precipitation based on this model.

#### 1.7 Results for both the linear model predictionand the GEV approach

```{r setup, include=TRUE}
return_level_10_gev <- return.level(gev_constant, return.period = 10)
return_level_10_gev

return_level_20_gev <- return.level(gev_constant, return.period = 20)
return_level_20_gev

return_level_50_gev <- return.level(gev_constant, return.period = 50)
return_level_50_gev

return_level_85_gev <- return.level(gev_constant, return.period = 85)
return_level_85_gev

historical_values <- yearly_max$MaxPrecipitation

exceed_10_gev <- sum(historical_values > return_level_10_gev)
exceed_10_gev

exceed_20_gev <- sum(historical_values > return_level_20_gev)
exceed_20_gev

exceed_50_gev <- sum(historical_values > return_level_50_gev)
exceed_50_gev

exceed_85_gev <- sum(historical_values > return_level_85_gev)
exceed_85_gev

linear_predictions <- predict(linear_model, newdata = yearly_max)

exceed_10_linear <- sum(linear_predictions > return_level_10_gev)
exceed_10_linear

exceed_20_linear <- sum(linear_predictions > return_level_20_gev)
exceed_20_linear

exceed_50_linear <- sum(linear_predictions > return_level_50_gev)
exceed_50_linear

exceed_85_linear <- sum(linear_predictions > return_level_85_gev)
exceed_85_linear

```

As expected, for the 10-year return level using the GEV model, there are 6 historical values where the maximum precipitation exceeds the 10-year return level.
For the 20-year return level, there are 4 exceedances, while there are 2 exceedances for the 50-year return level and 1 exceedance for the 85-year return level.
These results indicate that the GEV model performs well, capturing the frequency of extreme precipitation values accurately and in line with what is expected for rare events.
In contrast, the linear model shows 0 exceedances across all return levels, meaning it fails to account for extreme values.
This suggests that the linear model is unsuitable for predicting and modeling extreme events, as it does not adequately capture the distribution's tails

#### 1.8 Return period of 100 mm of precipitation.

```{r setup, include=TRUE}
threshold_precipitation <- 100
params <- gev_constant$results

cdf_100 <- pevd(threshold_precipitation, loc = params$par[1], scale = params$par[2], shape = params$par[3], type = "GEV")

return_period_100mm <- 1 / (1 - cdf_100)
return_period_100mm
```

Precipitation of 100 mm or more can occur once every 71.77 years.

#### 1.9 Probability that there will be a day in the next year when the precipitation exceeds 150 mm.

```{r setup, include=TRUE}
threshold_precipitation <- 150
params <- gev_constant$results

cdf_150 <- pevd(threshold_precipitation, loc = params$par[1], scale = params$par[2], shape = params$par[3], type = "GEV")

prob_exceed_150 <- 1 - cdf_150
prob_exceed_150

prob_not_exceed_any_day <- (1 - prob_exceed_150) ^ 365

prob_exceed_150_year <- 1 - prob_not_exceed_any_day
prob_exceed_150_year

```

The probability of exceeding 150 mm on any day is .0642% and the probability of at least one day in the next year is 20.9%.

### Part 2: Peaks-over-threshold approach

```{r}
rain_df <- read.csv("Precipitation_lausanne_full.csv")
```

#### 2.1 Display a time series plot of the daily precipitation across the data range

```{r}
# Convert 'Date' column to Date type
rain_df$Date <- as.Date(rain_df$Date, format="%m/%d/%Y")

# Plot time series of daily precipitation
ggplot(rain_df, aes(x = Date, y = Precipitation)) +
  geom_line(color = "#62A39F") +
  labs(title = "Daily Precipitation in Lausanne",
       x = "Date",
       y = "Precipitation (mm)") +
  theme_minimal()
```

#### 2.2 We want to model the high precipitation levels using the POT approach. First step is choosing a threshold. Draw Mean Residual Life Plot (for example using mrlplot in POT library) for the full range of your data. Choose a reasonable threshold. In the plot from part a) highlight the data that exceeds this threshold.

```{r}
# Mean Residual Life Plot to choose a threshold
mrlplot(rain_df$Precipitation, main = "Mean Residual Life Plot for Precipitation Data")
```

Between 20 and 40, the plot is relatively stable, with no strong upwards or downwards trend, and the mean excess remains quite constant.
In the region around 45-50, the mean excess shows larger fluctuations and the graph starts to act more erratic.
Therefore, we choose 40 as the threshold value.

```{r}
# Based on the plot, choose a reasonable threshold
threshold <- 40

# Highlight data exceeding the threshold on the time series plot
rain_df$ExceedsThreshold <- ifelse(rain_df$Precipitation > threshold, "Above Threshold", "Below Threshold")

ggplot(rain_df, aes(x = Date, y = Precipitation)) +
  geom_line(color = "#62A39F") +
  geom_point(aes(color = ExceedsThreshold), size = 1.5) +
  scale_color_manual(values = c("Above Threshold" = "#2683C6", "Below Threshold" = "#62A39F")) +
  labs(title = "Precipitation with Highlighted Exceedances",
       x = "Date",
       y = "Precipitation (mm)") +
  theme_minimal()

```

#### 2.3 it a GPD for the data exceeding the threshold and draw a diagnostic plot. Is it a reasonable fit? (Hint: if not, you may reconsider the choice of the threshold)

```{r}
# Fit a GPD to the data exceeding the threshold
exceedances <- rain_df$Precipitation[rain_df$Precipitation > threshold]
fit <- fitgpd(exceedances, threshold)

# Diagnostic plots to assess the fit
par(mfrow = c(2, 2))
plot(fit, npy = 1)
```

With a threshold value of 40, we get the following diagnostic for our model:

-   Probability plot: while our model tends to slightly overestimate low values (below 0.4), it seems overall reliable.

-   QQ-Plot: the fit is generally good.
    However, we notice an extreme value in the upper tail that is not properly captured by the model.
    Depending on the application, this could be an issue.

-   Density Plot: despite some slight variation, our fitted values align generally well with the model.

-   Return Level Plot: despite some slight variation in the 20-50 years period, the fit is generally good.

#### 2.4 Using the fitted model, compute the 10-year, 20-year, 50-year and 85-year return levels.

```{r}
# Fit the GPD using the extRemes package
fit_extremes <- fevd(rain_df$Precipitation, threshold = threshold, type = "GP")

# Return periods for 10, 20, 50, and 85 years
return_periods <- c(10, 20, 50, 85)

# Calculate return levels for these return periods
return_levels <- return.level(fit_extremes, return.period = return_periods)

# Print return levels
print(return_levels)
```

#### 2.5 Using the fitted model, compute the return period of 100 mm of precipitation.

```{r}
# Extract parameters from the fitted GPD model using extRemes
shape <- fit_extremes$results$par["shape"]
scale <- fit_extremes$results$par["scale"]
threshold <- fit_extremes$threshold
precipitation_level <- 100
# Compute the empirical exceedance rate above the chosen threshold
exceed_rate <- sum(rain_df$Precipitation > threshold) / nrow(rain_df)  # daily fraction of exceedances
# Convert to yearly exceedance rate (assuming ~365 days/year)
yearly_exceedance_rate <- exceed_rate * 365

# Calculate the probability of exceeding 'precipitation_level' on any given day
# Given that for y = precipitation_level - threshold:
# P(X > precipitation_level) = exceed_rate * (1 + shape * (y/scale))^(-1/shape)
y <- precipitation_level - threshold
p_exceed_100mm <- exceed_rate * (1 + shape * (y / scale))^(-1/shape)

# Convert this probability into a return period:
# Return period in days = 1 / daily probability
return_period_days <- 1 / p_exceed_100mm

# Convert the return period from days to years
return_period_years <- return_period_days / 365

print(paste0(">100mm rain every ", round(return_period_years, 2), " years"))
```

#### 2.6 Using the fitted model, compute the probability that there will be a day in the next year when the precipitation exceeds 150 mm.

```{r}
# Define the precipitation level we are interested in (150 mm)
precipitation_level_150 <- 150

# Probability that precipitation > threshold
p_exceed <- mean(rain_df$Precipitation > threshold) 
# Conditional probability above threshold for >150 mm
p_conditional <- 1 - pgpd(precipitation_level_150 - threshold, scale = scale, shape = shape) 
# Unconditional probability
prob_150mm <- p_exceed * p_conditional

# Print the probability
print(paste0("Probability that there is a day with >150mm rain next year is ", prob_150mm))
```

#### 2.7 Compare the results with the block maxima method. Explain the drawbacks and advantages of using the POT approach compared to the block maxima method. Which method do you prefer?

Interestingly, our approach to the POT and Block Maxima methods yield similar (in the same range) results for the return period of \>100mm rain.
However, we find widely different results when computing the probability that there will be a day in the next year when the precipitation exceed 150mm.

This could be due to several factor.
The first hypothesis is that, because the POT method is better at modelling extreme events than the block maxima (one of the advantages), the probablity found by the block maxima method widely over-estimate the risk.

The second hypothesis is that the threshold selection for the POT method (40) introduced some bias in the distribution, therefore leading to high variance in estimates (Drawback of the POT, subjective choice, yet critical).

### Part 3

#### 3.1 Upload the Geneva temperature data. Plot the data. Subset the data for the summer months (June to September).

```{r}
temp_df <- read.csv("Geneva_temperature.csv")
```

```{r}
# Convert the data types if necessary (ensuring 'Year', 'Month', and 'Day' are numeric)
temp_df$Date <- as.Date(with(temp_df, paste(Year, Month, Day, sep = "-")), "%Y-%m-%d")

# Plot the full time series of daily average temperatures in Geneva
ggplot(temp_df, aes(x = Date, y = AvgTemperature)) +
  geom_line(color = "dark blue") +
  labs(title = "Daily Average Temperature in Geneva",
       x = "Date",
       y = "Average Temperature (°C)") +
  theme_minimal()

```

```{r}
# Subset the data for summer months (June to September)
summer_df <- temp_df %>% filter(Month >= 6 & Month <= 9)

ggplot(summer_df, aes(x = Date, y = AvgTemperature)) +
  geom_line(colour = "Orange") + 
  labs(title = "Daily Average Temperature in Geneva, Summer",
       x = "Date",
       y = "Average Temperature (°C)") +
  theme_minimal()
```

#### 3.2 Compute the extremal index of the subsetted series with appropriatelly chosen threshold (for example, you can use extremalindex function in extRemes package). Do the extremes occur in clusters? What is the probability that if the temperature today is extreme (above the chosen threshold) then tomorrow will be also extreme?

```{r}
# Choose an appropriate threshold for extreme temperature
threshold_temp <- 20

# Compute the extremal index for the subsetted summer data
extremal_index_result <- extremalindex(summer_df$AvgTemperature, threshold_temp)

# Print the extremal index
extremal_index_result
```

With a threshold of 20 degrees Celcius, our extremal index is close to 0 (0.15), indicating that extreme temperature tend to happen in blocks (clusters).

With a threshold of 25 degrees Celcius, our extremal index is still low (0,27), further proving that extreme temperatures (\>25 C) happen in clusters.
This can be illustrated for example by heatwaves in Summer.

#### 3.3 Decluster the data using a suitable threshold. Plot the resulting declustered data. (Hint: you may want to use decluster function in the extRemes package.)

```{r}
# Decluster the summer temperature data based on the chosen threshold
declustered_data <- decluster(summer_df$AvgTemperature, threshold = threshold_temp)

# Convert declustered data back into a data frame with corresponding dates for plotting
declustered_df <- data.frame(Date = summer_df$Date, AvgTemperature = declustered_data)

# Plot the declustered data
ggplot(declustered_df, aes(x = Date, y = AvgTemperature)) +
  geom_line(color = "#2683C6") +
  labs(title = "Declustered Summer Temperatures in Geneva",
       x = "Date",
       y = "Declustered Temperature (°C)") +
  theme_minimal()
```

#### 3.4 Fit a Generalized Pareto Distribution (GPD) to the data, both raw and declustered. Compare the models and compute 10-year return level.

```{r}
# Fit a GPD to the raw summer temperature data using extRemes
fit_gpd_raw <- fevd(summer_df$AvgTemperature, threshold = threshold_temp, type = "GP")

# Fit a GPD to the declustered summer temperature data using extRemes
fit_gpd_declust <- fevd(declustered_df$AvgTemperature, threshold = threshold_temp, type = "GP")

# Calculate the 10-year return level for both models
return_level_raw <- return.level(fit_gpd_raw, return.period = 10)
return_level_declust <- return.level(fit_gpd_declust, return.period = 10)

# Print the return levels
print(return_level_raw)
print(return_level_declust)
```

The raw summer data return level for 10-years is 29.23 degrees Celcius.
It means that on average, the temperature of 29.23 C will be exceeded on average every 10 years.

Without clustering of extreme events (the declustered data), the 10-years return level is 29.16 degrees Celcius.
This is very close to the raw data and indicates that the clustering of extreme values does not have a significant impact on the 10-years return level.

## Practical 3

### 1. Data Pre-processing

```{r, message =FALSE}
install.packages(c("extRemes", "lubridate", "dplyr", "ggplot2", "kableExtra"))


library("xts")
library("quantmod")
library(kableExtra)
library(ggplot2)
library(dplyr)
library(tseries)
library(PerformanceAnalytics)
library(extRemes) 
library(lubridate) 
library(evir)
library(extRemes)
library(ismev)
library(tidyr)             

```

```{r}
raw_data <- read.csv("INDEX_US_XNAS_COMP.csv")

# Prepare for time-series
raw_data$Date <- as.Date(raw_data$Date, format = "%m/%d/%Y")
raw_data$Open <- as.numeric(gsub(",", "", raw_data$Open))
raw_data <- raw_data[order(raw_data$Date), ]

# Computing the daily returns
raw_data$Return <- c(NA, diff(raw_data$Open) / head(raw_data$Open, -1) * 100)
raw_data <- raw_data[-1, ]

# Check if any missing values
print(paste0("NAs in column : ",colSums(is.na(raw_data))))

# Time-Series of Nasdaq
nasdaq_ts <- ts(raw_data$Open, 
              start = c(as.numeric(format(min(raw_data$Date), "%Y")), 
                        as.numeric(format(min(raw_data$Date), "%j"))),
              frequency = 365) # Daily data

open_xts <- xts(raw_data$Open, order.by = raw_data$Date)
```

#### 1.1 Plot the time-series

```{r}
plot(nasdaq_ts, main = "Open Prices Time Series", ylab = "Open Price", xlab = "Time", xaxt = "n")
axis(1, at = unique(floor(time(nasdaq_ts))))

print(summary(nasdaq_ts))
```

#### 1.2 Computing the daily returns

```{r}
# 1. Compute the daily returns
raw_data$Return <- c(NA, -diff(log(raw_data$Open)) * 100)  # Negative log returns in percentage
raw_data <- na.omit(raw_data)  # Remove NA values from the first row due to diff

# Plot daily returns
plot(raw_data$Date, raw_data$Return, type = "l", col = "#1CADE4", xlab = "Date", ylab = "Negative Log Returns (%)", main = "Negative Log Returns of Nasdaq")

```

The summary of the daily returns shows a wide range of values, with a minimum return of -6.84% and a maximum return of 7.13%, indicating significant volatility.
The mean return is 0.06%, suggesting a mild positive average return, while the median return is higher at 0.18%, indicating that most daily returns are slightly positive.

#### 1.3 Check for stationarity

```{r}
# 2. Check for stationarity using Augmented Dickey-Fuller test
adf_test <- adf.test(raw_data$Return)
print(adf_test)
if (adf_test$p.value > 0.05) {
  cat("The time series is non-stationary. Differencing will be performed to achieve stationarity.\n")
  raw_data$Stationary_Return <- diff(raw_data$Return)
  raw_data <- na.omit(raw_data)
}

```

The daily returns are stationary (pval \< 0.05),.

#### 1.4 Check normality

```{r}
# 3. Test for normality
shapiro_test <- shapiro.test(raw_data$Return)
print(shapiro_test)
if (shapiro_test$p.value > 0.05) {
  cat("The distribution appears to be normal. Proceeding with parametric VaR calculation.\n")
} else {
  cat("The distribution does not appear to be normal. Caution is advised when using parametric VaR methods.\n")
}

# Plot histogram and Q-Q plot to visually inspect normality
par(mfrow = c(1, 2))
hist(raw_data$Return, breaks = 30, main = "Histogram of Negative Log Returns", xlab = "Negative Log Returns (%)", col = "#2683C6")
qqnorm(raw_data$Return, main = "Q-Q Plot of Negative Log Returns")
qqline(raw_data$Return, col = "#42BA97")
par(mfrow = c(1, 1))
```

The daily returns are not normally distributed.
Therefore, using parametric Value at Risk and Expected Shortfall could result in under/overestimating the risk.
We will therefore use the historical VaR and ES moving forward.

### 2. Extreme Values Theory

#### 2.1 Block Maxima

Weekly blocks were created by grouping the data into weeks based on the `Date` column.
For each week, the **maximum return** (largest loss) was calculated, representing the **worst weekly loss** due to the negative log transformation used in the return calculation.

```{r}
# Step 1: Prepare Weekly Block Maxima
raw_data$YearWeek <- format(raw_data$Date, "%Y-%U")  # Create Year-Week grouping

# Calculate block maxima (largest losses) for each week
block_maxima_weekly <- raw_data %>%
  group_by(YearWeek) %>%
  summarize(BlockMax = max(Return, na.rm = TRUE))  # Weekly maximum returns (largest losses)

# Step 2: Fit the GEV distribution
gev_fit_weekly <- fevd(block_maxima_weekly$BlockMax, type = "GEV")

# Step 3: Summarize GEV parameters
gev_summary <- summary(gev_fit_weekly)
print(gev_summary)


```

The table shows the **weekly block maxima** (largest losses) for the first six weeks in the dataset:

```{r}
  # Weekly periods
# Round the BlockMax values to 4 decimals
block_maxima_weekly$BlockMax <- round(block_maxima_weekly$BlockMax, 4)

# Select only the first 6 rows of the table
head_block_max <- head(block_maxima_weekly)

# Display the table (only the first 6 rows)
head_block_max


```

Each row highlights the **largest weekly loss** for the index.
For example, in the 49th week of 2020, the worst weekly loss was **2.7711**, which reflects a significant drop in returns during that week.

```{r}
# Step 4: Visualize Weekly Block Maxima
ggplot(block_maxima_weekly, aes(x = as.Date(paste0(YearWeek, "-1"), format = "%Y-%U-%u"), y = BlockMax)) +
  geom_line(color = "#27CED7") +
  geom_point(color = "#3E8853") +
  labs(
    title = "Weekly Block Maxima of Losses",
    x = "Week",
    y = "Block Maxima (Largest Losses)"
  ) +
  theme_minimal()

```

The graph visualizes the weekly **largest losses** over time, highlighting periods of heightened market stress, such as in 2022, where weekly losses reached their most extreme values.
Outside of 2022, the general trend indicates that most weekly losses are less severe, reflecting a relatively stable risk profile during those periods.
This visualization provides a clear timeline of extreme losses, offering insights into when and where the market experienced the most significant volatility.

#### 2.1.2 VaR and ES calculation

The table compares Value at Risk (VaR) and Expected Shortfall (ES) for 95% and 99% confidence levels using historical and parametric approaches.

```{r}
gev_params <- gev_fit_weekly$results$par
location <- gev_params["location"]  # Mu
scale <- gev_params["scale"]        # Sigma
shape <- gev_params["shape"]        # Xi

# Definir niveles de confianza
confidence_levels <- c(0.95, 0.99)

# Función para calcular VaR y ES usando la distribución GEV
gev_var_es <- function(location, scale, shape, confidence_level) {
  # Calcular VaR
  VaR <- location + (scale / shape) * ((-log(1 - confidence_level))^(-shape) - 1)
  
  # Calcular ES
  ES <- location + (scale / shape) * (
    (1 / (1 - shape)) * ((-log(1 - confidence_level))^(-shape)) - 1
  )
  
  return(list(VaR = -VaR, ES = -ES))  # Valores negativos para reflejar pérdidas
}

# Inicializar un data frame para almacenar los resultados
block_maxima_results <- data.frame(
  Confidence_Level = confidence_levels,
  Historical_VaR = numeric(length(confidence_levels)),
  Historical_ES = numeric(length(confidence_levels)),
  Parametric_VaR = numeric(length(confidence_levels)),
  Parametric_ES = numeric(length(confidence_levels))
)

# Calcular VaR y ES históricos y paramétricos para cada nivel de confianza
for (i in seq_along(confidence_levels)) {
  cl <- confidence_levels[i]
  
  # VaR histórico
  historical_var <- quantile(block_maxima_weekly$BlockMax, probs = 1 - cl, na.rm = TRUE)
  
  # ES histórico
  historical_es <- mean(block_maxima_weekly$BlockMax[block_maxima_weekly$BlockMax <= historical_var], na.rm = TRUE)
  
  # VaR y ES paramétricos usando la distribución GEV
  parametric_results <- gev_var_es(location, scale, shape, cl)
  parametric_var <- parametric_results$VaR
  parametric_es <- parametric_results$ES
  
  # Almacenar resultados
  block_maxima_results$Historical_VaR[i] <- historical_var
  block_maxima_results$Historical_ES[i] <- historical_es
  block_maxima_results$Parametric_VaR[i] <- parametric_var
  block_maxima_results$Parametric_ES[i] <- parametric_es
}


block_maxima_results %>%
  kable("html", caption = "Value at Risk and Expected Shortfall (Historical and Parametric)") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

The parametric approach shows a wider range, especially in ES at 95%, where the estimated loss magnitude is **significantly higher.** This discrepancy reflects the sensitivity of the parametric method, which assumes a heavier-tailed GEV model.
However, since the underlying distribution is known to be non-normal, the parametric estimates might overstate risks, particularly at lower confidence levels, emphasizing the need for careful model validation.

#### 2.2 Peaks

By applying the Peaks Over Threshold (POT) method using the Open prices, we've estimated the Value at Risk (VaR) and Expected Shortfall (ES) for extreme events in the Nasdaq Composite Index.
This method provides an accurate and robust assessment of risk for extreme losses.

#### 2.2.1. POT Parametric

**Threshold Selection**

The choice of threshold is crucial in the POT method.
An appropriate threshold balances bias and variance in parameter estimates.The mean excess plot helps identify a suitable threshold where the mean excess over the threshold is linear.
Based on the plot, a suitable threshold would be around -2 to 0, where the mean excess stabilizes and becomes linear.

```{r}
# Mean Excess Plot
meplot(-raw_data$Return, main = "Mean Excess Plot", col = "#42BA97")
```

The parameter stability plots provided a guidance to select a threshold.
The region 2.2 and 2.8 shows stability, making it reliable range for analyzing extreme events.

```{r}
# Define a sequence of thresholds
thresholds <- seq(quantile(-raw_data$Return, 0.90, na.rm = TRUE),
                  quantile(-raw_data$Return, 0.99, na.rm = TRUE),
                  length.out = 50)

# Initialize vectors to store parameters
xi_values <- numeric(length(thresholds))
beta_values <- numeric(length(thresholds))

# Fit the GPD for each threshold and store parameters
for (i in seq_along(thresholds)) {
  threshold <- thresholds[i]
  # Fit GPD only if threshold is not NA
  if (!is.na(threshold)) {
    fit <- gpd(-raw_data$Return, threshold = threshold)
    xi_values[i] <- fit$par.ests["xi"]
    beta_values[i] <- fit$par.ests["beta"]
  } else {
    xi_values[i] <- NA
    beta_values[i] <- NA
  }
}

# Create a data frame for plotting
gpd_params_df <- data.frame(
  Threshold = thresholds,
  Xi = xi_values,
  Beta = beta_values
)

# Plot parameter stability with custom colors
par(mfrow = c(1, 2))  # Arrange plots side by side

# Plot for Xi (Stability of Xi) with a custom color
plot(gpd_params_df$Threshold, gpd_params_df$Xi, type = "l",
     col = "#2683C6",  # Custom color for Xi
     xlab = "Threshold", ylab = "Xi",
     main = "Stability of Xi", lwd = 2)

# Plot for Beta (Stability of Beta) with a custom color
plot(gpd_params_df$Threshold, gpd_params_df$Beta, type = "l",
     col = "#42BA97",  # Custom color for Beta
     xlab = "Threshold", ylab = "Beta",
     main = "Stability of Beta", lwd = 2)

# Reset plotting parameters
par(mfrow = c(1, 1))

```

```{r}
# Step 1: Set the threshold based on stability analysis
u <- 2.5  # Chosen threshold from stability region

# Step 2: Extract exceedances over the threshold
excesses <- -raw_data$Return[raw_data$Return > u] - u

# Print the threshold and some of the exceedances for verification
print(paste("Selected Threshold (u):", u))
print(head(excesses))

```

Fitting the Generalized Pareto Distribution (GPD):

```{r}
# Fit the GPD to the exceedances
gpd_fit <- gpd(-raw_data$Return, threshold = u)

# Summarize the fitted model
kable(summary(gpd_fit))

```

Estimating Value at Risk (VaR) and Expected Shortfall (ES)

```{r}
# Estimated parameters
xi <- as.numeric(gpd_fit$par.ests["xi"])
beta <- as.numeric(gpd_fit$par.ests["beta"])

# Sample sizes
N <- length(raw_data$Return)
N_exc <- sum(-raw_data$Return > u)

# Confidence levels
p_levels <- c(0.99, 0.995, 0.999)

# Calculate VaR
VaR_POT <- sapply(p_levels, function(p) {
  VaR <- u + (beta / xi) * (((N_exc / (N * (1 - p)))^xi - 1))
  return(-VaR)  # Convert back to negative return
})

# Create a data frame for results
VaR_POT_df <- data.frame(
  Confidence_Level = p_levels,
  VaR = VaR_POT
)

# Display results
kable(print(VaR_POT_df))

```

```{r}
# Calculate ES
ES_POT <- sapply(1:length(p_levels), function(i) {
  p <- p_levels[i]
  VaR_p <- -VaR_POT[i]  # Use positive value for calculation
  ES <- (VaR_p / (1 - xi)) + ((beta - xi * u) / (1 - xi))
  return(-ES)  # Convert back to negative return
})

# Add ES to the data frame
VaR_POT_df$ES <- ES_POT

# Display results
kable(print(VaR_POT_df))

```

The analysis highlights the increasing severity of potential losses as confidence levels rise, with VaR and ES growing more extreme at 99.9% confidence.

**Stop-Loss Strategy Based on POT VaR**

```{r}
# Add VaR levels to the dataset
raw_data$VaR_POT_99 <- VaR_POT_df$VaR[VaR_POT_df$Confidence_Level == 0.99]

# Generate stop-loss signals
raw_data$Stop_Loss_POT_99 <- raw_data$Return <= raw_data$VaR_POT_99

# Count the number of triggers
num_triggers_POT_99 <- sum(raw_data$Stop_Loss_POT_99, na.rm = TRUE)
cat("Number of stop-loss activations with POT at 99%:", num_triggers_POT_99, "\n")

```

According to the next plot, which compares the cumulative returns of two strategies.
early (2021-2022) the performance of both strategies si similar, as extreme loss events are limited.But in late 2022, during a period of high market volatility, the stop-loss strategy reduce the significant losses, as it ca be observed between the two lines.

```{r}
# Cumulative returns without stop-loss
raw_data$Cumulative_Return_No_Stop <- cumsum(raw_data$Return)

# Cumulative returns with stop-loss POT
raw_data$Cumulative_Return_With_Stop_POT_99 <- cumsum(ifelse(raw_data$Stop_Loss_POT_99, 0, raw_data$Return))

# Plot cumulative returns
library(ggplot2)

# Create a data frame for cumulative returns
cumulative_returns <- data.frame(
  Date = rep(raw_data$Date, 2),
  Cumulative_Return = c(raw_data$Cumulative_Return_No_Stop,
                        raw_data$Cumulative_Return_With_Stop_POT_99),
  Strategy = rep(c("Without Stop-Loss", "With Stop-Loss (POT 99%)"),
                 each = nrow(raw_data))
)

# Custom colors for the strategies
strategy_colors <- c("Without Stop-Loss" = "#1CADE4",  # Light Blue
                     "With Stop-Loss (POT 99%)" = "#42BA97")  # Green

# Plot cumulative returns with custom colors
ggplot(cumulative_returns, aes(x = Date, y = Cumulative_Return, color = Strategy)) +
  geom_line(size = 0.8) +
  scale_color_manual(values = strategy_colors) +  # Apply custom colors
  labs(
    title = "Cumulative Returns with and without Stop-Loss Based on POT",
    x = "Date",
    y = "Cumulative Return (%)"
  ) +
  theme_minimal() +
  theme(
    legend.title = element_blank(),  # Remove legend title
    legend.position = "bottom"       # Position legend at the bottom
  )

```

The POT-based stop-loss strategy significantly outperforms the non-stop-loss approach in terms of long-term cumulative returns, highlighting its practicality for managing extreme risk.

#### 2.2.2. POT Historic

```{r}
# Define the threshold (e.g., 95th percentile of negative returns)
threshold <- quantile(-raw_data$Return, 0.95, na.rm = TRUE)

# Print the selected threshold
cat("Selected Threshold:", threshold, "\n")

```

```{r}
# Extract excesses over the threshold
excesses <- -raw_data$Return[-raw_data$Return > threshold] - threshold

# Print the first few exceedances
cat("Excesses over the threshold:\n")
print(head(excesses))

```

Calculating the VaR historical with POT

```{r}
# Define confidence levels
confidence_levels <- c(0.99, 0.995, 0.999)

# Calculate Historical VaR using POT
historical_var_pot <- sapply(confidence_levels, function(cl) {
  quantile(excesses, probs = cl, na.rm = TRUE) + threshold  # Add back the threshold
})

# Print the Historical VaR results
historical_var_pot_df <- data.frame(
  Confidence_Level = paste0(confidence_levels * 100, "%"),
  Historical_VaR_POT = -historical_var_pot  # Convert back to negative returns
)

```

Calculating ES Historical with POT

```{r}
# Calculate Historical ES using POT
historical_es_pot <- sapply(1:length(confidence_levels), function(i) {
  var_threshold <- historical_var_pot[i] - threshold  # Find the excess threshold
  mean(excesses[excesses >= var_threshold], na.rm = TRUE) + threshold  # Add back the threshold
})

# Add ES to the DataFrame
historical_var_pot_df$Historical_ES_POT <- -historical_es_pot  # Convert back to negative returns

```

The table presents the **Historical VaR** and **Historical ES** calculated using the Peaks Over Threshold (POT) method at different confidence levels (99%, 99.5%, and 99.9%).
The **VaR values** increase as the confidence level rises, reflecting higher potential losses as the analysis focuses on more extreme events.

The **ES value** captures the average of these extreme losses, which is slightly worse than the 99.9% VaR.
These results provide a conservative and realistic estimate of potential losses, making the Historical POT approach useful for assessing extreme risk scenarios without relying on parametric assumptions.

```{r}
# Combine results into a DataFrame
historical_var_pot_df <- data.frame(
  Confidence_Level = paste0(confidence_levels * 100, "%"),
  Historical_VaR_POT = -historical_var_pot,  # Convert back to negative returns
  Historical_ES_POT = -historical_es_pot    # Convert back to negative returns
)

# Print the final results
print("Historical VaR and ES using POT:")
print(historical_var_pot_df)
```

The VaR grows as the confidence level increases, which is expected because higher confidence levels capture rarer, more extreme events.
The 99% VaR indicates that losses will not exceed 6.37% on 99 out of 100 trading days, while the ES shows that when losses exceed the VaR, the average loss is 6.88%.
As confidence levels increase, the VaR captures progressively worse losses (up to 6.83% at 99.9% confidence), showing the potential for larger losses in rarer scenarios.

### 3. Models Comparison

```{r}
confidence_levels <- c(0.95, 0.99, 0.999)

gev_var_es <- function(location, scale, shape, confidence_level) {

  VaR <- location + (scale / shape) * ((-log(1 - confidence_level))^(-shape) - 1)
  
  ES <- location + (scale / shape) * (
    (1 / (1 - shape)) * ((-log(1 - confidence_level))^(-shape)) - 1
  )
  
  return(list(VaR = -VaR, ES = -ES))  
}

gev_params <- gev_fit_weekly$results$par
location <- gev_params["location"]
scale <- gev_params["scale"]
shape <- gev_params["shape"]

gev_results <- data.frame(
  Confidence_Level = confidence_levels,
  GEV_VaR = sapply(confidence_levels, function(cl) gev_var_es(location, scale, shape, cl)$VaR),
  GEV_ES = sapply(confidence_levels, function(cl) gev_var_es(location, scale, shape, cl)$ES)
)

N <- length(raw_data$Return)
N_exc <- sum(-raw_data$Return > u)  
beta <- as.numeric(gpd_fit$par.ests["beta"])
xi <- as.numeric(gpd_fit$par.ests["xi"])

VaR_POT <- sapply(confidence_levels, function(p) {
  u + (beta / xi) * (((N_exc / (N * (1 - p)))^xi - 1))
})

ES_POT <- sapply(1:length(confidence_levels), function(i) {
  p <- confidence_levels[i]
  VaR_p <- u + (beta / xi) * (((N_exc / (N * (1 - p)))^xi - 1))
  (VaR_p / (1 - xi)) + ((beta - xi * u) / (1 - xi))
})

pot_results <- data.frame(
  Confidence_Level = confidence_levels,
  POT_VaR = -VaR_POT,  
  POT_ES = -ES_POT     
)

comparison <- gev_results %>%
  left_join(pot_results, by = "Confidence_Level") %>%
  rename(
    GEV_VaR = GEV_VaR,
    GEV_ES = GEV_ES,
    POT_VaR = POT_VaR,
    POT_ES = POT_ES
  )

comparison_long <- comparison %>%
  pivot_longer(cols = -Confidence_Level, names_to = "Metric", values_to = "Value")

ggplot(comparison_long, aes(x = factor(Confidence_Level), y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Comparison of VaR and ES for GEV and POT Models",
    x = "Confidence Level",
    y = "Value",
    fill = "Metric"
  ) +
  scale_fill_manual(values = c(
    "GEV_ES" = "#3E8853",
    "GEV_VaR" = "#1CADE4",
    "POT_ES" = "#27CED7",
    "POT_VaR" = "#42BA97"
  )) +
  theme_minimal()


print("Comparison of VaR and ES between GEV and POT Models:")
print(comparison)
```

Looking at the results from the comparison between GEV and POT models' VaR and ES, it is clear that POT model captures more extreme losses than GEV, which may be useful for risk averse scenarios.
GEV model may be underestimating the risks given that it only considers block maxima, while POT is considering ALL exceedances above the threshold, making the model more sensitive to extreme events.

Given these results, for three different confidence intervals, POT model is better than GEV, especially for financial events which require a more rigourous analysis rather than the simplicity GEV offers.

### 4. Conclusion

In order to evaluate extreme risks in the Nasdaq Composite Index using Extreme Value Theory, two models were implemented: Block Maxima (GEV), which focuses on capturing the largest loss in blocks and Peaks Over Threshold (POT), which analyzes losses exceeding a threshold.
Through the Stop Loss Strategy, POT models demonstrated its ability to reduce losses during volatile periods, specifically in 2022.
Both models provided VaR and ES estimates for different confidence levels in order to determine which one is better.
The results demonstrate POT model performs better for financial events.
GEV provides a simple summary within predefined time blocks, so it underestimates extreme events because it only considers the largest loss within each block.
This model is more appropriate for lower risk environments which require computational simplicity.
In the other hand, POT is more sensitive to extreme events so it produced more conservative VaR and ES estimates, especially for 99% and 99.9% confidence levels.
This model works better for volatile environments such as the financial one.
