---
title: "Practical 3"
output: html_document
date: "2024-11-19"
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Data Pre-processing

```{r, message =FALSE}
install.packages(c("extRemes", "lubridate", "dplyr", "ggplot2", "kableExtra"))

library("xts")
library("quantmod")
library(kableExtra)
library(ggplot2)
library(dplyr)
library(tseries)
library(PerformanceAnalytics)
library(extRemes) 
library(lubridate) 
library(evir)


```

```{r}
raw_data <- read.csv("INDEX_US_XNAS_COMP.csv")

# Prepare for time-series
raw_data$Date <- as.Date(raw_data$Date, format = "%m/%d/%Y")
raw_data$Open <- as.numeric(gsub(",", "", raw_data$Open))
raw_data <- raw_data[order(raw_data$Date), ]

# Computing the daily returns
raw_data$Return <- c(NA, diff(raw_data$Open) / head(raw_data$Open, -1) * 100)
raw_data <- raw_data[-1, ]

# Check if any missing values
print(paste0("NAs in column : ",colSums(is.na(raw_data))))

# Time-Series of Nasdaq
nasdaq_ts <- ts(raw_data$Open, 
              start = c(as.numeric(format(min(raw_data$Date), "%Y")), 
                        as.numeric(format(min(raw_data$Date), "%j"))),
              frequency = 365) # Daily data

open_xts <- xts(raw_data$Open, order.by = raw_data$Date)
```

#### 1.1 Plot the time-series

```{r}
plot(nasdaq_ts, main = "Open Prices Time Series", ylab = "Open Price", xlab = "Time", xaxt = "n")
axis(1, at = unique(floor(time(nasdaq_ts))))

print(summary(nasdaq_ts))
```

#### 1.2 Computing the daily returns

```{r}
# 1. Compute the daily returns
raw_data$Return <- c(NA, -diff(log(raw_data$Open)) * 100)  # Negative log returns in percentage
raw_data <- na.omit(raw_data)  # Remove NA values from the first row due to diff

# Plot daily returns
plot(raw_data$Date, raw_data$Return, type = "l", col = "#1CADE4", xlab = "Date", ylab = "Negative Log Returns (%)", main = "Negative Log Returns of Nasdaq")

```

The summary of the daily returns shows a wide range of values, with a minimum return of -6.84% and a maximum return of 7.13%, indicating significant volatility.
The mean return is 0.06%, suggesting a mild positive average return, while the median return is higher at 0.18%, indicating that most daily returns are slightly positive.

## 2. Value at Risk (VaR)

#### 2.1 Check for stationarity

```{r}
# 2. Check for stationarity using Augmented Dickey-Fuller test
adf_test <- adf.test(raw_data$Return)
print(adf_test)
if (adf_test$p.value > 0.05) {
  cat("The time series is non-stationary. Differencing will be performed to achieve stationarity.\n")
  raw_data$Stationary_Return <- diff(raw_data$Return)
  raw_data <- na.omit(raw_data)
}

```

The daily returns are stationary (pval \< 0.05),.

#### 2.2 Check normality

```{r}
# 3. Test for normality
shapiro_test <- shapiro.test(raw_data$Return)
print(shapiro_test)
if (shapiro_test$p.value > 0.05) {
  cat("The distribution appears to be normal. Proceeding with parametric VaR calculation.\n")
} else {
  cat("The distribution does not appear to be normal. Caution is advised when using parametric VaR methods.\n")
}

# Plot histogram and Q-Q plot to visually inspect normality
par(mfrow = c(1, 2))
hist(raw_data$Return, breaks = 30, main = "Histogram of Negative Log Returns", xlab = "Negative Log Returns (%)", col = "lightblue")
qqnorm(raw_data$Return, main = "Q-Q Plot of Negative Log Returns")
qqline(raw_data$Return, col = "red")
par(mfrow = c(1, 1))
```

The daily returns are not normally distributed.
Therefore, using parametric Value at Risk and Expected Shortfall could result in under/overestimating the risk.
We will therefore use the historical VaR and ES moving forward.

#### 2.3 Calculation of VaR

```{r}
# 4. Calculate Value at Risk (VaR)
confidence_levels <- c(0.95, 0.99)

# Historical VaR calculation
historical_var <- numeric(length(confidence_levels))
for (i in seq_along(confidence_levels)) {
  cl <- confidence_levels[i]
  historical_var[i] <- quantile(raw_data$Return, probs = 1 - cl)
}

# Output Historical VaR results
for (i in seq_along(confidence_levels)) {
  cat(sprintf("Historical VaR at %.0f%% confidence level: %.2f%%\n", 
              confidence_levels[i] * 100, historical_var[i]))
}
# Parametric VaR calculation
mu <- mean(raw_data$Return)  # Mean of returns
sigma <- sd(raw_data$Return) # Standard deviation of returns
confidence_levels <- c(0.95, 0.99)

# Calculate VaR for each confidence level
parametric_var <- numeric(length(confidence_levels))
for (i in seq_along(confidence_levels)) {
  z_alpha <- qnorm(1 - confidence_levels[i])  # Z value for the confidence level
  parametric_var[i] <- mu + z_alpha * sigma
}

# Output Parametric VaR results
for (i in seq_along(confidence_levels)) {
  cat(sprintf("Parametric VaR at %.0f%% confidence level: %.2f%%\n", 
              confidence_levels[i] * 100, parametric_var[i]))
}

```

We can interpret these results as: "With probability 5% (1%), the daily returns will be smaller than **-2.19% (-3.35%).**

T**he Historical VaR values indicate that, with 95% confidence, the worst expected daily loss is -2.46%, and with 99% confidence, it is -3.93%. In comparison, the Parametric VaR estimates the worst daily loss at -2.38% for the 95% confidence level and -3.35%** for the 99% confidence level.
While both methods suggest similar risk levels, the Historical VaR provides a more direct representation of past market behavior, whereas the Parametric VaR relies on statistical assumptions, offering a slightly more conservative estimate of potential losses.

```{r}


# Visualize VaR levels against daily returns
plot(raw_data$Date, raw_data$Return, type = "l", col = "#1CADE4", xlab = "Date", ylab = "Negative Log Returns (%)", main = "Negative Log Returns with VaR Levels")
abline(h = quantile(raw_data$Return, probs = 1 - 0.95), col = "#42BA97", lty = 2, lwd = 2)
abline(h = quantile(raw_data$Return, probs = 1 - 0.99), col = "#3E8853", lty = 2, lwd = 2)
legend("topright", legend = c("Negative Log Returns", "Historical VaR 95%", "Historical VaR 99%"), col = c("#1CADE4", "#42BA97", "#3E8853"), lty = c(1, 2, 2))

```

```{r}
# 5. Calculate Expected Shortfall (ES)
# Historical ES calculation
historical_es <- numeric(length(confidence_levels))
for (i in seq_along(confidence_levels)) {
  cl <- confidence_levels[i]
  historical_es[i] <- mean(raw_data$Return[raw_data$Return <= quantile(raw_data$Return, probs = 1 - cl)])
}

# Compare VaR and ES values
cat("\nComparison of VaR and ES:\n")
for (cl in confidence_levels) {
  h_var <- quantile(raw_data$Return, probs = 1 - cl)
  h_es <- mean(raw_data$Return[raw_data$Return <= historical_var])
  cat(sprintf("At %.0f%% confidence level:\n", cl * 100))
  cat(sprintf("  Historical VaR: %.2f%%, Historical ES: %.2f%%\n", h_var, h_es))
}
```

We can interpret these results as: "When the daily returns are lower than -2.19% (-3.35%), it is expected to be **-2.91**% (-4.29%).

```{r}
# 5. Combine VaR and ES into a data frame for comparison

levels <- data.frame(
  Confidence_Level = paste0(confidence_levels * 100, "%"),
  Historical_VaR = historical_var,
  Historical_ES = historical_es
)

# Compare VaR and ES values
cat("\nComparison of VaR and ES:\n")
for (i in seq_along(confidence_levels)) {
  cat(sprintf("At %.0f%% confidence level:\n", confidence_levels[i] * 100))
  cat(sprintf("  Historical VaR: %.2f%%, Historical ES: %.2f%%\n", historical_var[i], historical_es[i]))
}

historical_var <- levels[,1:2]
historical_es <- levels[,c(1,3)]
```

The Historical Expected Shortfall (ES) values a**re -3.39% at the 95% confidence level and -4.78% at the 99% confidence level, indicating the average loss that occurs beyond the VaR threshold. The Parametric Expected Shortfall (ES) values are 2.86% at the 95% confidence level and 3.71% at the 99% confidence level, which are positive, reflecting a different outcome. The parametric metho**d appears to indicate potential gains or less severe losses, as it assumes returns follow a normal distribution, which may not capture extreme losses as effectively as the historical approach.
This disparity highlights the limitations of the parametric model in accurately estimating the true risk of extreme events.

In conclusion, the Historical method provides a more conservative and realistic measure of risk, especially in tail events, by incorporating real past data, while the Parametric method, based on the assumption of normality, tends to underestimate extreme risk.

## 3. The Stop-Loss Strategy

### 3.1 Count the Triggers

```{r}
# Add VaR and ES thresholds to the data
raw_data$VaR_95 <- historical_var[1,2]
raw_data$VaR_99 <- historical_var[2,2]
raw_data$ES_95 <- historical_es[1,2]
raw_data$ES_99 <- historical_es[2,2]

# Identify when stop-loss is triggered
raw_data$Stop_Loss_95 <- raw_data$Return <= raw_data$VaR_95
raw_data$Stop_Loss_99 <- raw_data$Return <= raw_data$VaR_99
raw_data$Stop_Loss_ES_95 <- raw_data$Return <= raw_data$ES_95
raw_data$Stop_Loss_ES_99 <- raw_data$Return <= raw_data$ES_99

# Count triggers for each strategy
trigger_counts <- colSums(raw_data[, c("Stop_Loss_95", "Stop_Loss_99", "Stop_Loss_ES_95", "Stop_Loss_ES_99")])
names(trigger_counts) <- c("VaR 95%", "VaR 99%", "ES 95%", "ES 99%")

print("Number of Stop-Loss Activations:")
print(trigger_counts)

```

The stop-loss strategy was triggered a different number of times depending on the chosen risk measure (VaR or ES) and confidence level.
The 95% confidence level resulted in significantly more activations compared to the 99% confidence level, as expected.
This reflects the more conservative nature of the 99% threshold, which corresponds to rarer and more extreme market movements.
Expected Shortfall (ES) triggered fewer activations than VaR at the same confidence level, demonstrating its focus on the average of the extreme tail events rather than a specific quantile.

### 3.2 Cumulative Returns Analysis

```{r}
# Cumulative returns without stop-loss
raw_data$Cumulative_Return_No_Stop <- cumsum(raw_data$Return)

# Cumulative returns with stop-loss for each strategy
raw_data$Cumulative_Return_With_Stop_95 <- cumsum(ifelse(raw_data$Stop_Loss_95, 0, raw_data$Return))
raw_data$Cumulative_Return_With_Stop_99 <- cumsum(ifelse(raw_data$Stop_Loss_99, 0, raw_data$Return))
raw_data$Cumulative_Return_With_Stop_ES_95 <- cumsum(ifelse(raw_data$Stop_Loss_ES_95, 0, raw_data$Return))
raw_data$Cumulative_Return_With_Stop_ES_99 <- cumsum(ifelse(raw_data$Stop_Loss_ES_99, 0, raw_data$Return))

# Plot Cumulative returns without stop-loss and with stop-loss for each strategy
data.frame(
  Date = rep(raw_data$Date, 5),
  Cumulative_Return = c(
    raw_data$Cumulative_Return_No_Stop,
    raw_data$Cumulative_Return_With_Stop_95,
    raw_data$Cumulative_Return_With_Stop_99,
    raw_data$Cumulative_Return_With_Stop_ES_95,
    raw_data$Cumulative_Return_With_Stop_ES_99
  ),
  Strategy = rep(c("Without Stop-Loss", 
                   "With Stop-Loss (VaR 95%)", 
                   "With Stop-Loss (VaR 99%)", 
                   "With Stop-Loss (ES 95%)", 
                   "With Stop-Loss (ES 99%)"), 
                 each = nrow(raw_data))
) %>% ggplot( aes(x = Date, y = Cumulative_Return, color = Strategy)) +
  geom_line() +
  scale_color_manual(values = c(
    "Without Stop-Loss" = "blue", 
    "With Stop-Loss (VaR 95%)" = "red", 
    "With Stop-Loss (VaR 99%)" = "darkred", 
    "With Stop-Loss (ES 95%)" = "green", 
    "With Stop-Loss (ES 99%)" = "darkgreen"
  )) +
  labs(
    title = "Cumulative Returns with and without Stop-Loss Strategies",
    x = "Date",
    y = "Cumulative Return"
  ) +
  theme_minimal() +
  theme(legend.title = element_blank())

```

The analysis reveals a significant difference between the cumulative returns of strategies with and without stop-loss.
Specifically, the strategy with the stop-loss set at VaR 95% yielded the highest cumulative return, reaching 230, while the strategy without any stop-loss resulted in the lowest cumulative return at 57.
This suggests that implementing a VaR-based stop-loss strategy helps to mitigate large losses, leading to a more favorable overall performance.
The stop-loss strategy limits significant downturns by cutting losses early, which can be beneficial in volatile market conditions.
The stop-loss triggered during sharp declines in the portfolio, effectively preventing deeper drawdowns.
This is evident as the cumulative return with the stop-loss strategy is consistently higher, demonstrating the protective nature of the stop-loss in limiting the impact of negative returns.
However, one consideration when using a stop-loss is the potential for missing significant rebounds after a sharp downturn.
The stop-loss strategy may have exited positions at lower prices, missing the opportunity to capitalize on rebounds.

### How well do the previous developed models predict actual losses?

In order to evaluate the accuracy of the developed models, we will perform backtesting to both VaR. Backtesting checks for each day if the actual returns are worse than the predictions and counts the number of breaches (days when the actual losses exceed VaR). For the model to be accurate, the violation rate should be close to 5% (1%). 
For ES, we will perform tail risk prediction, which compares the actual extreme losses to the ES prediction. 
```{r}
var_95 <- historical_var[1] 
var_99 <- historical_var[2]  

breaches_95 <- sum(raw_data$Return < var_95)  
breaches_99 <- sum(raw_data$Return < var_99)  

total_days <- nrow(raw_data)

violation_rate_95 <- breaches_95 / total_days
violation_rate_99 <- breaches_99 / total_days

expected_violation_95 <- 1 - 0.95 
expected_violation_99 <- 1 - 0.99

cat("Backtesting VaR:\n")
cat(sprintf("95%% VaR: Actual Violation Rate = %.2f%%, Expected = %.2f%%\n", 
            violation_rate_95 * 100, expected_violation_95 * 100))
cat(sprintf("99%% VaR: Actual Violation Rate = %.2f%%, Expected = %.2f%%\n", 
            violation_rate_99 * 100, expected_violation_99 * 100))
```

Both 95% and 99% VaR models underestimated the actual frequency of extreme losses, this is because Historical VaR is too conservative and captures most losses without living room to actual violations. 

```{r}
es_95 <- historical_es$Historical_ES[1] 
es_99 <- historical_es$Historical_ES[2] 

mean_tail_loss_95 <- mean(raw_data$Return[raw_data$Return < var_95])
mean_tail_loss_99 <- mean(raw_data$Return[raw_data$Return < var_99])

cat("Expected Shortfall Validation:\n")
cat(sprintf("95%% ES: Predicted = %.2f%%, Actual Mean Loss = %.2f%%\n", es_95, mean_tail_loss_95))
cat(sprintf("99%% ES: Predicted = %.2f%%, Actual Mean Loss = %.2f%%\n", es_99, mean_tail_loss_99))


```

The ES model provides a realistic measure of tail risk, however, the average loss will be 2.91% (4.29%), which in practice is much higher than the real average loss. The model is too conservative, because it assumes much worse losses than what actually occurred in order to ensure safety. However, a conservative model can be inefficient because it leads investors to avoid risks unnecessarily. 

```{r}
parametric_var_95 <- parametric_var[1]
parametric_var_99 <- parametric_var[2]

parametric_breaches_95 <- sum(raw_data$Return < parametric_var_95)
parametric_breaches_99 <- sum(raw_data$Return < parametric_var_99)

parametric_violation_rate_95 <- parametric_breaches_95 / total_days
parametric_violation_rate_99 <- parametric_breaches_99 / total_days

cat("Comparison of Historical and Parametric VaR:\n")
cat(sprintf("95%% Historical VaR: Violation Rate = %.2f%%\n", violation_rate_95 * 100))
cat(sprintf("95%% Parametric VaR: Violation Rate = %.2f%%\n", parametric_violation_rate_95 * 100))
cat(sprintf("99%% Historical VaR: Violation Rate = %.2f%%\n", violation_rate_99 * 100))
cat(sprintf("99%% Parametric VaR: Violation Rate = %.2f%%\n", parametric_violation_rate_99 * 100))
```
Parametric Var for both 95% and 99% confidence levels are more realistic than the historical ones. For 95%, the violation rate gets closer to the expected one but still under predicts the actual frequency of violations. For 99%, it almost matches the expected 1%, which is fine. So, historical VaR is more conservative, leaving no tail events to breach the threshold, while parametric VaR performs better especially at 99% confidence. 

```{r}
breach_data <- data.frame(
  Confidence_Level = c("95%", "99%"),
  Historical_VaR = c(violation_rate_95, violation_rate_99) * 100,
  Parametric_VaR = c(parametric_violation_rate_95, parametric_violation_rate_99) * 100
)

ggplot(breach_data, aes(x = Confidence_Level)) +
  geom_bar(aes(y = Historical_VaR, fill = "Historical VaR"), stat = "identity", position = "dodge") +
  geom_bar(aes(y = Parametric_VaR, fill = "Parametric VaR"), stat = "identity", position = "dodge") +
  labs(title = "VaR Violation Rates", y = "Violation Rate (%)", x = "Confidence Level") +
  theme_minimal()


```
## 4. Extreme Values Theory

### 4.1 Block Maxima

Weekly blocks were created by grouping the data into weeks based on the `Date` column.
For each week, the **maximum return** (largest loss) was calculated, representing the **worst weekly loss** due to the negative log transformation used in the return calculation.

```{r}
# Step 1: Prepare Weekly Block Maxima
raw_data$YearWeek <- format(raw_data$Date, "%Y-%U")  # Create Year-Week grouping

# Calculate block maxima (largest losses) for each week
block_maxima_weekly <- raw_data %>%
  group_by(YearWeek) %>%
  summarize(BlockMax = max(Return, na.rm = TRUE))  # Weekly maximum returns (largest losses)

# Step 2: Fit the GEV distribution
gev_fit_weekly <- fevd(block_maxima_weekly$BlockMax, type = "GEV")

# Step 3: Summarize GEV parameters
gev_summary <- summary(gev_fit_weekly)
print(gev_summary)


```

The table shows the **weekly block maxima** (largest losses) for the first six weeks in the dataset:

```{r}
  # Weekly periods
# Round the BlockMax values to 4 decimals
block_maxima_weekly$BlockMax <- round(block_maxima_weekly$BlockMax, 4)

# Select only the first 6 rows of the table
head_block_max <- head(block_maxima_weekly)

# Display the table (only the first 6 rows)
head_block_max


```

Each row highlights the **largest weekly loss** for the index.
For example, in the 49th week of 2020, the worst weekly loss was **2.7711**, which reflects a significant drop in returns during that week.

```{r}
# Step 4: Visualize Weekly Block Maxima
ggplot(block_maxima_weekly, aes(x = as.Date(paste0(YearWeek, "-1"), format = "%Y-%U-%u"), y = BlockMax)) +
  geom_line(color = "#27CED7") +
  geom_point(color = "#3E8853") +
  labs(
    title = "Weekly Block Maxima of Losses",
    x = "Week",
    y = "Block Maxima (Largest Losses)"
  ) +
  theme_minimal()

```

The graph visualizes the weekly **largest losses** over time, highlighting periods of heightened market stress, such as in 2022, where weekly losses reached their most extreme values.
Outside of 2022, the general trend indicates that most weekly losses are less severe, reflecting a relatively stable risk profile during those periods.
This visualization provides a clear timeline of extreme losses, offering insights into when and where the market experienced the most significant volatility.

### 4.2 Peaks

By applying the Peaks Over Threshold (POT) method using the Open prices, we've estimated the Value at Risk (VaR) and Expected Shortfall (ES) for extreme events in the Nasdaq Composite Index.
This method provides an accurate and robust assessment of risk for extreme losses.

**Threshold Selection**

The choice of threshold is crucial in the POT method.
An appropriate threshold balances bias and variance in parameter estimates.The mean excess plot helps identify a suitable threshold where the mean excess over the threshold is linear.
Based on the plot, a suitable threshold would be around -2 to 0, where the mean excess stabilizes and becomes linear.

```{r}
# Mean Excess Plot
meplot(-raw_data$Return, main = "Mean Excess Plot", col = "#42BA97")
```

The parameter stability plots provided a guidance to select a threshold.
The region 2.2 and 2.8 shows stability, making it reliable range for analyzing extreme events.

```{r}
# Define a sequence of thresholds
thresholds <- seq(quantile(-raw_data$Return, 0.90, na.rm = TRUE),
                  quantile(-raw_data$Return, 0.99, na.rm = TRUE),
                  length.out = 50)

# Initialize vectors to store parameters
xi_values <- numeric(length(thresholds))
beta_values <- numeric(length(thresholds))

# Fit the GPD for each threshold and store parameters
for (i in seq_along(thresholds)) {
  threshold <- thresholds[i]
  # Fit GPD only if threshold is not NA
  if (!is.na(threshold)) {
    fit <- gpd(-raw_data$Return, threshold = threshold)
    xi_values[i] <- fit$par.ests["xi"]
    beta_values[i] <- fit$par.ests["beta"]
  } else {
    xi_values[i] <- NA
    beta_values[i] <- NA
  }
}

# Create a data frame for plotting
gpd_params_df <- data.frame(
  Threshold = thresholds,
  Xi = xi_values,
  Beta = beta_values
)

# Plot parameter stability with custom colors
par(mfrow = c(1, 2))  # Arrange plots side by side

# Plot for Xi (Stability of Xi) with a custom color
plot(gpd_params_df$Threshold, gpd_params_df$Xi, type = "l",
     col = "#2683C6",  # Custom color for Xi
     xlab = "Threshold", ylab = "Xi",
     main = "Stability of Xi", lwd = 2)

# Plot for Beta (Stability of Beta) with a custom color
plot(gpd_params_df$Threshold, gpd_params_df$Beta, type = "l",
     col = "#42BA97",  # Custom color for Beta
     xlab = "Threshold", ylab = "Beta",
     main = "Stability of Beta", lwd = 2)

# Reset plotting parameters
par(mfrow = c(1, 1))

```

```{r}
# Step 1: Set the threshold based on stability analysis
u <- 2.5  # Chosen threshold from stability region

# Step 2: Extract exceedances over the threshold
excesses <- -raw_data$Return[raw_data$Return > u] - u

# Print the threshold and some of the exceedances for verification
print(paste("Selected Threshold (u):", u))
print(head(excesses))

```

Fitting the Generalized Pareto Distribution (GPD):

```{r}
# Fit the GPD to the exceedances
gpd_fit <- gpd(-raw_data$Return, threshold = u)

# Summarize the fitted model
kable(summary(gpd_fit))

```

Estimating Value at Risk (VaR) and Expected Shortfall (ES)

```{r}
# Estimated parameters
xi <- as.numeric(gpd_fit$par.ests["xi"])
beta <- as.numeric(gpd_fit$par.ests["beta"])

# Sample sizes
N <- length(raw_data$Return)
N_exc <- sum(-raw_data$Return > u)

# Confidence levels
p_levels <- c(0.99, 0.995, 0.999)

# Calculate VaR
VaR_POT <- sapply(p_levels, function(p) {
  VaR <- u + (beta / xi) * (((N_exc / (N * (1 - p)))^xi - 1))
  return(-VaR)  # Convert back to negative return
})

# Create a data frame for results
VaR_POT_df <- data.frame(
  Confidence_Level = p_levels,
  VaR = VaR_POT
)

# Display results
kable(print(VaR_POT_df))

```

```{r}
# Calculate ES
ES_POT <- sapply(1:length(p_levels), function(i) {
  p <- p_levels[i]
  VaR_p <- -VaR_POT[i]  # Use positive value for calculation
  ES <- (VaR_p / (1 - xi)) + ((beta - xi * u) / (1 - xi))
  return(-ES)  # Convert back to negative return
})

# Add ES to the data frame
VaR_POT_df$ES <- ES_POT

# Display results
kable(print(VaR_POT_df))

```

The analysis highlights the increasing severity of potential losses as confidence levels rise, with VaR and ES growing more extreme at 99.9% confidence.

**Stop-Loss Strategy Based on POT VaR**

```{r}
# Add VaR levels to the dataset
raw_data$VaR_POT_99 <- VaR_POT_df$VaR[VaR_POT_df$Confidence_Level == 0.99]

# Generate stop-loss signals
raw_data$Stop_Loss_POT_99 <- raw_data$Return <= raw_data$VaR_POT_99

# Count the number of triggers
num_triggers_POT_99 <- sum(raw_data$Stop_Loss_POT_99, na.rm = TRUE)
cat("Number of stop-loss activations with POT at 99%:", num_triggers_POT_99, "\n")

```

According to the next plot, which compares the cumulative returns of two strategies.
early (2021-2022) the performance of both strategies si similar, as extreme loss events are limited.But in late 2022, during a period of high market volatility, the stop-loss strategy reduce the significant losses, as it ca be observed between the two lines.

```{r}
# Cumulative returns without stop-loss
raw_data$Cumulative_Return_No_Stop <- cumsum(raw_data$Return)

# Cumulative returns with stop-loss POT
raw_data$Cumulative_Return_With_Stop_POT_99 <- cumsum(ifelse(raw_data$Stop_Loss_POT_99, 0, raw_data$Return))

# Plot cumulative returns
library(ggplot2)

# Create a data frame for cumulative returns
cumulative_returns <- data.frame(
  Date = rep(raw_data$Date, 2),
  Cumulative_Return = c(raw_data$Cumulative_Return_No_Stop,
                        raw_data$Cumulative_Return_With_Stop_POT_99),
  Strategy = rep(c("Without Stop-Loss", "With Stop-Loss (POT 99%)"),
                 each = nrow(raw_data))
)

# Custom colors for the strategies
strategy_colors <- c("Without Stop-Loss" = "#1CADE4",  # Light Blue
                     "With Stop-Loss (POT 99%)" = "#42BA97")  # Green

# Plot cumulative returns with custom colors
ggplot(cumulative_returns, aes(x = Date, y = Cumulative_Return, color = Strategy)) +
  geom_line(size = 0.8) +
  scale_color_manual(values = strategy_colors) +  # Apply custom colors
  labs(
    title = "Cumulative Returns with and without Stop-Loss Based on POT",
    x = "Date",
    y = "Cumulative Return (%)"
  ) +
  theme_minimal() +
  theme(
    legend.title = element_blank(),  # Remove legend title
    legend.position = "bottom"       # Position legend at the bottom
  )

```

The POT-based stop-loss strategy significantly outperforms the non-stop-loss approach in terms of long-term cumulative returns, highlighting its practicality for managing extreme risk.

### Models Comparison

1. Historical VaR: estimates the worst case loss based on historical returns, it is simple and reflects historical market behavior, however it assumes the future will behave as the past and does not consider the extreme tail behavior beyond the historical range. 
2. Parametric VaR: uses statistical assumptions to estimate the worst case loss, instead of using historical data. It is useful for stable markets with normally distributed returns, however the assumption for normality may underestimate tail risks and also may not work properly in volatile market conditions. 
3. Expected Shortfall: estimates the avergae loss in the worst case scenario when losses exceed the VaR threshold. It considers tail risks beyond VaR, so relies on the same data or assumptions as the VaR model (same issues). It can also be overly conservative. 
4. Block Maxima Model: groups data into fixed time intervals and extracts the largest loss in each block. It is useful because it focuses on extreme events without considering a specific threshold. However, it only extracts one value per block, other extreme values are ignored. 
5. Peaks Over Threshold Method: focuses only on extreme losses that exceed a certain threshold. It provides more accurate estimates of tail risks for extreme events and generates VaR and ES for very high confiedence levels. However, selecting a threshold requires a lot of attentions. 

Previously, we determined that parametric VaR was the less conservative and hence the more realistic, especially for a 99% confidence level. In order to determine which one is better between Block Maxima and POT: 

```{r}
raw_data$YearWeek <- format(raw_data$Date, "%Y-%U")
block_maxima_weekly <- raw_data %>%
  group_by(YearWeek) %>%
  summarize(BlockMax = max(Return, na.rm = TRUE))
gev_fit <- fevd(block_maxima_weekly$BlockMax, type = "GEV")

threshold <- 2.5
exceedances <- -raw_data$Return[raw_data$Return > threshold] - threshold

gpd_fit <- gpd(-raw_data$Return, threshold = threshold)

return_periods <- c(10, 50, 100)
gev_return_levels <- return.level(gev_fit, return.periods = return_periods)

N <- length(raw_data$Return)
N_exc <- length(exceedances)
xi <- gpd_fit$par.ests["xi"]
beta <- gpd_fit$par.ests["beta"]

confidence_levels <- 1 - 1 / return_periods

pot_return_levels <- sapply(confidence_levels, function(p) {
  threshold + (beta / xi) * (((N_exc / (N * (1 - p)))^xi - 1))
})

cat("Peaks Over Threshold (POT) Return Levels:\n")
print(data.frame(Return_Period = return_periods, Return_Level = pot_return_levels))

qqplot(
  qgev(
    ppoints(length(block_maxima_weekly$BlockMax)),
    loc = gev_fit$results$par["location"], 
    scale = gev_fit$results$par["scale"], 
    shape = gev_fit$results$par["shape"]  
  ),
  block_maxima_weekly$BlockMax,
  main = "QQ-Plot for Block Maxima (GEV)",
  xlab = "Theoretical Quantiles",
  ylab = "Observed Block Maxima"
)
abline(0, 1, col = "red")

plot(gev_fit, type = "rl", main = "Return Level Plot (Block Maxima)")

pot_return_levels_plot <- data.frame(
  Return_Period = return_periods,
  Return_Level = pot_return_levels
)

ggplot(pot_return_levels_plot, aes(x = Return_Period, y = Return_Level)) +
  geom_point(color = "blue") +
  geom_line(color = "blue") +
  labs(title = "Return Level Plot (POT)", x = "Return Period (Years)", y = "Return Level") +
  theme_minimal()
```
Considering the QQ Plot for Block Maxima, most of the points align with the diagonal which suggests that the GDP is an appropriate fit to the exceedances, however the deviation at the tails suggest a slight under or overestimation of extreme values. Looking at the Return Level Plot for Block Maxima, we can also see that the GEV model has a good fit since there are higher extreme values for less frequent events. 

Considering the Return Level Plot for POT, show that the return levels are lower compared to the GEV model, which indicates the GDP is less conservative for extreme events. 

From the plots, we can conclude overall both have a good fit to the data, however the GEV model can be more conservative, predicting higher extreme values than POT. 

### Recommendations: 
1. The expected shortfall model should be used with careful attention given that it is overly conservative and could lead to mistakes in the risk management decisions. 
