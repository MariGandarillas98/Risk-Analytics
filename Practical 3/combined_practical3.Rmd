---
title: "Practical 3"
output:
  pdf_document: default
  html_document: default
date: "2024-11-19"
editor_options:
  markdown:
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Data Pre-processing

```{r, message =FALSE}
install.packages(c("extRemes", "lubridate", "dplyr", "ggplot2", "kableExtra"))


library("xts")
library("quantmod")
library(kableExtra)
library(ggplot2)
library(dplyr)
library(tseries)
library(PerformanceAnalytics)
library(extRemes) 
library(lubridate) 
library(evir)
library(extRemes)
```

```{r}
raw_data <- read.csv("INDEX_US_XNAS_COMP.csv")

# Prepare for time-series
raw_data$Date <- as.Date(raw_data$Date, format = "%m/%d/%Y")
raw_data$Open <- as.numeric(gsub(",", "", raw_data$Open))
raw_data <- raw_data[order(raw_data$Date), ]

# Computing the daily returns
raw_data$Return <- c(NA, diff(raw_data$Open) / head(raw_data$Open, -1) * 100)
raw_data <- raw_data[-1, ]

# Check if any missing values
print(paste0("NAs in column : ",colSums(is.na(raw_data))))

# Time-Series of Nasdaq
nasdaq_ts <- ts(raw_data$Open, 
              start = c(as.numeric(format(min(raw_data$Date), "%Y")), 
                        as.numeric(format(min(raw_data$Date), "%j"))),
              frequency = 365) # Daily data

open_xts <- xts(raw_data$Open, order.by = raw_data$Date)
```

#### 1.1 Plot the time-series

```{r}
plot(nasdaq_ts, main = "Open Prices Time Series", ylab = "Open Price", xlab = "Time", xaxt = "n")
axis(1, at = unique(floor(time(nasdaq_ts))))

print(summary(nasdaq_ts))
```

#### 1.2 Computing the daily returns

```{r}
# 1. Compute the daily returns
raw_data$Return <- c(NA, -diff(log(raw_data$Open)) * 100)  # Negative log returns in percentage
raw_data <- na.omit(raw_data)  # Remove NA values from the first row due to diff

# Plot daily returns
plot(raw_data$Date, raw_data$Return, type = "l", col = "#1CADE4", xlab = "Date", ylab = "Negative Log Returns (%)", main = "Negative Log Returns of Nasdaq")

```

The summary of the daily returns shows a wide range of values, with a minimum return of -6.84% and a maximum return of 7.13%, indicating significant volatility.
The mean return is 0.06%, suggesting a mild positive average return, while the median return is higher at 0.18%, indicating that most daily returns are slightly positive.

## 2. Value at Risk (VaR)

#### 2.1 Check for stationarity

```{r}
# 2. Check for stationarity using Augmented Dickey-Fuller test
adf_test <- adf.test(raw_data$Return)
print(adf_test)
if (adf_test$p.value > 0.05) {
  cat("The time series is non-stationary. Differencing will be performed to achieve stationarity.\n")
  raw_data$Stationary_Return <- diff(raw_data$Return)
  raw_data <- na.omit(raw_data)
}

```

The daily returns are stationary (pval \< 0.05),.

#### 2.2 Check normality

```{r}
# 3. Test for normality
shapiro_test <- shapiro.test(raw_data$Return)
print(shapiro_test)
if (shapiro_test$p.value > 0.05) {
  cat("The distribution appears to be normal. Proceeding with parametric VaR calculation.\n")
} else {
  cat("The distribution does not appear to be normal. Caution is advised when using parametric VaR methods.\n")
}

# Plot histogram and Q-Q plot to visually inspect normality
par(mfrow = c(1, 2))
hist(raw_data$Return, breaks = 30, main = "Histogram of Negative Log Returns", xlab = "Negative Log Returns (%)", col = "lightblue")
qqnorm(raw_data$Return, main = "Q-Q Plot of Negative Log Returns")
qqline(raw_data$Return, col = "red")
par(mfrow = c(1, 1))
```

The daily returns are not normally distributed.
Therefore, using parametric Value at Risk and Expected Shortfall could result in under/overestimating the risk.
We will therefore use the historical VaR and ES moving forward.

#### 2.3 Calculation of VaR

```{r}
# 4. Calculate Value at Risk (VaR)
confidence_levels <- c(0.95, 0.99)

# Historical VaR calculation
historical_var <- numeric(length(confidence_levels))
for (i in seq_along(confidence_levels)) {
  cl <- confidence_levels[i]
  historical_var[i] <- quantile(raw_data$Return, probs = 1 - cl)
}

# Output Historical VaR results
for (i in seq_along(confidence_levels)) {
  cat(sprintf("Historical VaR at %.0f%% confidence level: %.2f%%\n", 
              confidence_levels[i] * 100, historical_var[i]))
}
# Parametric VaR calculation
mu <- mean(raw_data$Return)  # Mean of returns
sigma <- sd(raw_data$Return) # Standard deviation of returns
confidence_levels <- c(0.95, 0.99)

# Calculate VaR for each confidence level
parametric_var <- numeric(length(confidence_levels))
for (i in seq_along(confidence_levels)) {
  z_alpha <- qnorm(1 - confidence_levels[i])  # Z value for the confidence level
  parametric_var[i] <- mu + z_alpha * sigma
}

# Output Parametric VaR results
for (i in seq_along(confidence_levels)) {
  cat(sprintf("Parametric VaR at %.0f%% confidence level: %.2f%%\n", 
              confidence_levels[i] * 100, parametric_var[i]))
}

```

We can interpret these results as: "With probability 5% (1%), the daily returns will be smaller than **-2.19% (-3.35%).**

T**he Historical VaR values indicate that, with 95% confidence, the worst expected daily loss is -2.46%, and with 99% confidence, it is -3.93%. In comparison, the Parametric VaR estimates the worst daily loss at -2.38% for the 95% confidence level and -3.35%** for the 99% confidence level.
While both methods suggest similar risk levels, the Historical VaR provides a more direct representation of past market behavior, whereas the Parametric VaR relies on statistical assumptions, offering a slightly more conservative estimate of potential losses.

```{r}


# Visualize VaR levels against daily returns
plot(raw_data$Date, raw_data$Return, type = "l", col = "#1CADE4", xlab = "Date", ylab = "Negative Log Returns (%)", main = "Negative Log Returns with VaR Levels")
abline(h = quantile(raw_data$Return, probs = 1 - 0.95), col = "#42BA97", lty = 2, lwd = 2)
abline(h = quantile(raw_data$Return, probs = 1 - 0.99), col = "#3E8853", lty = 2, lwd = 2)
legend("topright", legend = c("Negative Log Returns", "Historical VaR 95%", "Historical VaR 99%"), col = c("#1CADE4", "#42BA97", "#3E8853"), lty = c(1, 2, 2))

```

```{r}
# 5. Calculate Expected Shortfall (ES)
# Historical ES calculation
historical_es <- numeric(length(confidence_levels))
for (i in seq_along(confidence_levels)) {
  cl <- confidence_levels[i]
  historical_es[i] <- mean(raw_data$Return[raw_data$Return <= quantile(raw_data$Return, probs = 1 - cl)])
}

# Compare VaR and ES values
cat("\nComparison of VaR and ES:\n")
for (cl in confidence_levels) {
  h_var <- quantile(raw_data$Return, probs = 1 - cl)
  h_es <- mean(raw_data$Return[raw_data$Return <= historical_var])
  cat(sprintf("At %.0f%% confidence level:\n", cl * 100))
  cat(sprintf("  Historical VaR: %.2f%%, Historical ES: %.2f%%\n", h_var, h_es))
}
```

We can interpret these results as: "When the daily returns are lower than -2.19% (-3.35%), it is expected to be **-2.91**% (-4.29%).

```{r}
# 5. Combine VaR and ES into a data frame for comparison

levels <- data.frame(
  Confidence_Level = paste0(confidence_levels * 100, "%"),
  Historical_VaR = historical_var,
  Historical_ES = historical_es
)

# Compare VaR and ES values
cat("\nComparison of VaR and ES:\n")
for (i in seq_along(confidence_levels)) {
  cat(sprintf("At %.0f%% confidence level:\n", confidence_levels[i] * 100))
  cat(sprintf("  Historical VaR: %.2f%%, Historical ES: %.2f%%\n", historical_var[i], historical_es[i]))
}

historical_var <- levels[,1:2]
historical_es <- levels[,c(1,3)]
```

The Historical Expected Shortfall (ES) values a**re -3.39% at the 95% confidence level and -4.78% at the 99% confidence level, indicating the average loss that occurs beyond the VaR threshold. The Parametric Expected Shortfall (ES) values are 2.86% at the 95% confidence level and 3.71% at the 99% confidence level, which are positive, reflecting a different outcome. The parametric metho**d appears to indicate potential gains or less severe losses, as it assumes returns follow a normal distribution, which may not capture extreme losses as effectively as the historical approach.
This disparity highlights the limitations of the parametric model in accurately estimating the true risk of extreme events.

In conclusion, the Historical method provides a more conservative and realistic measure of risk, especially in tail events, by incorporating real past data, while the Parametric method, based on the assumption of normality, tends to underestimate extreme risk.

## 3. The Stop-Loss Strategy

### 3.1 Count the Triggers

```{r}
# Add VaR and ES thresholds to the data
raw_data$VaR_95 <- historical_var[1,2]
raw_data$VaR_99 <- historical_var[2,2]
raw_data$ES_95 <- historical_es[1,2]
raw_data$ES_99 <- historical_es[2,2]

# Identify when stop-loss is triggered
raw_data$Stop_Loss_95 <- raw_data$Return <= raw_data$VaR_95
raw_data$Stop_Loss_99 <- raw_data$Return <= raw_data$VaR_99
raw_data$Stop_Loss_ES_95 <- raw_data$Return <= raw_data$ES_95
raw_data$Stop_Loss_ES_99 <- raw_data$Return <= raw_data$ES_99

# Count triggers for each strategy
trigger_counts <- colSums(raw_data[, c("Stop_Loss_95", "Stop_Loss_99", "Stop_Loss_ES_95", "Stop_Loss_ES_99")])
names(trigger_counts) <- c("VaR 95%", "VaR 99%", "ES 95%", "ES 99%")

print("Number of Stop-Loss Activations:")
print(trigger_counts)

```

The stop-loss strategy was triggered a different number of times depending on the chosen risk measure (VaR or ES) and confidence level.
The 95% confidence level resulted in significantly more activations compared to the 99% confidence level, as expected.
This reflects the more conservative nature of the 99% threshold, which corresponds to rarer and more extreme market movements.
Expected Shortfall (ES) triggered fewer activations than VaR at the same confidence level, demonstrating its focus on the average of the extreme tail events rather than a specific quantile.

### 3.2 Cumulative Returns Analysis

```{r}
# Cumulative returns without stop-loss
raw_data$Cumulative_Return_No_Stop <- cumsum(raw_data$Return)

# Cumulative returns with stop-loss for each strategy
raw_data$Cumulative_Return_With_Stop_95 <- cumsum(ifelse(raw_data$Stop_Loss_95, 0, raw_data$Return))
raw_data$Cumulative_Return_With_Stop_99 <- cumsum(ifelse(raw_data$Stop_Loss_99, 0, raw_data$Return))
raw_data$Cumulative_Return_With_Stop_ES_95 <- cumsum(ifelse(raw_data$Stop_Loss_ES_95, 0, raw_data$Return))
raw_data$Cumulative_Return_With_Stop_ES_99 <- cumsum(ifelse(raw_data$Stop_Loss_ES_99, 0, raw_data$Return))

# Plot Cumulative returns without stop-loss and with stop-loss for each strategy
data.frame(
  Date = rep(raw_data$Date, 5),
  Cumulative_Return = c(
    raw_data$Cumulative_Return_No_Stop,
    raw_data$Cumulative_Return_With_Stop_95,
    raw_data$Cumulative_Return_With_Stop_99,
    raw_data$Cumulative_Return_With_Stop_ES_95,
    raw_data$Cumulative_Return_With_Stop_ES_99
  ),
  Strategy = rep(c("Without Stop-Loss", 
                   "With Stop-Loss (VaR 95%)", 
                   "With Stop-Loss (VaR 99%)", 
                   "With Stop-Loss (ES 95%)", 
                   "With Stop-Loss (ES 99%)"), 
                 each = nrow(raw_data))
) %>% ggplot( aes(x = Date, y = Cumulative_Return, color = Strategy)) +
  geom_line() +
  scale_color_manual(values = c(
    "Without Stop-Loss" = "blue", 
    "With Stop-Loss (VaR 95%)" = "red", 
    "With Stop-Loss (VaR 99%)" = "darkred", 
    "With Stop-Loss (ES 95%)" = "green", 
    "With Stop-Loss (ES 99%)" = "darkgreen"
  )) +
  labs(
    title = "Cumulative Returns with and without Stop-Loss Strategies",
    x = "Date",
    y = "Cumulative Return"
  ) +
  theme_minimal() +
  theme(legend.title = element_blank())

```

The analysis reveals a significant difference between the cumulative returns of strategies with and without stop-loss.
Specifically, the strategy with the stop-loss set at VaR 95% yielded the highest cumulative return, reaching 230, while the strategy without any stop-loss resulted in the lowest cumulative return at 57.
This suggests that implementing a VaR-based stop-loss strategy helps to mitigate large losses, leading to a more favorable overall performance.
The stop-loss strategy limits significant downturns by cutting losses early, which can be beneficial in volatile market conditions.
The stop-loss triggered during sharp declines in the portfolio, effectively preventing deeper drawdowns.
This is evident as the cumulative return with the stop-loss strategy is consistently higher, demonstrating the protective nature of the stop-loss in limiting the impact of negative returns.
However, one consideration when using a stop-loss is the potential for missing significant rebounds after a sharp downturn.
The stop-loss strategy may have exited positions at lower prices, missing the opportunity to capitalize on rebounds.

### How well do the previous developed models predict actual losses?

In order to evaluate the accuracy of the developed models, we will perform backtesting to VaR.
Backtesting checks each day if the actual returns are worse than the predictions and counts the number of breaches (days when the actual losses exceed VaR).
For the model to be accurate, the violation rate should be close to 5% (1%).
For ES, we will perform tail risk prediction, which compares the actual extreme losses to the ES prediction.

```{r}
var_95 <- historical_var[1] 
var_99 <- historical_var[2]  

breaches_95 <- sum(raw_data$Return < var_95)  
breaches_99 <- sum(raw_data$Return < var_99)  

total_days <- nrow(raw_data)

violation_rate_95 <- breaches_95 / total_days
violation_rate_99 <- breaches_99 / total_days

expected_violation_95 <- 1 - 0.95 
expected_violation_99 <- 1 - 0.99

cat("Backtesting VaR:\n")
cat(sprintf("95%% VaR: Actual Violation Rate = %.2f%%, Expected = %.2f%%\n", 
            violation_rate_95 * 100, expected_violation_95 * 100))
cat(sprintf("99%% VaR: Actual Violation Rate = %.2f%%, Expected = %.2f%%\n", 
            violation_rate_99 * 100, expected_violation_99 * 100))
```

Both 95% and 99% VaR models underestimated the actual frequency of extreme losses, this is because Historical VaR is too conservative and captures most losses within its range.
Historical VaR uses past data, and if this data already includes extreme events, then the model will predict large thresholds, leaving almost no room for violations.
In this sense, this model the low violation rates suggest the model captures more risk than necessary.

```{r}
varr_95 <- historical_var$Historical_VaR[historical_var$Confidence_Level == "95%"]
varr_95 <- as.numeric(varr_95)

varr_99 <- historical_var$Historical_VaR[historical_var$Confidence_Level == "99%"]
varr_99 <- as.numeric(varr_99)

plot(raw_data$Date, raw_data$Return, type = "l", col = "#1CADE4", 
     xlab = "Date", ylab = "Daily Returns (%)", main = "Daily Returns vs. VaR Thresholds")


abline(h = varr_95, col = "#FF6347", lty = 2, lwd = 2)  
abline(h = varr_99, col = "#32CD32", lty = 2, lwd = 2) 

legend("topright", legend = c("Daily Returns", "95% VaR", "99% VaR"), 
       col = c("#1CADE4", "#FF6347", "#32CD32"), lty = c(1, 2, 2), lwd = 2)

```

As seen in the graph, most daily returns are around -2% and 2%.
VaR 95% is around -2.19% and VaR 99% is around -3.35%, only few returns approach or breach the thresholds, meaning these leave no room for violations.
This model can be beneficial for risk averse strategies, but may lead to overestimation of risk buffers.

```{r}
es_95 <- historical_es$Historical_ES[1] 
es_99 <- historical_es$Historical_ES[2] 

mean_tail_loss_95 <- mean(raw_data$Return[raw_data$Return < var_95])
mean_tail_loss_99 <- mean(raw_data$Return[raw_data$Return < var_99])

cat("Expected Shortfall Validation:\n")
cat(sprintf("95%% ES: Predicted = %.2f%%, Actual Mean Loss = %.2f%%\n", es_95, mean_tail_loss_95))
cat(sprintf("99%% ES: Predicted = %.2f%%, Actual Mean Loss = %.2f%%\n", es_99, mean_tail_loss_99))


```

A good way to complement the Historical VaR is to rely on the ES, in order to understand the average severity losses beyond the thresholds.
The ES model provides a realistic measure of tail risk, the average loss will be 2.91% (4.29%), which means that the observed average loss for returns below the 95% (99%) VaR are less severe or no observed losses in the 99% case, indicating the model is overly conservative.
ES assumes extreme losses are much larger than the actual historical data.

```{r}
parametric_var_95 <- parametric_var[1]
parametric_var_99 <- parametric_var[2]

parametric_breaches_95 <- sum(raw_data$Return < parametric_var_95)
parametric_breaches_99 <- sum(raw_data$Return < parametric_var_99)

parametric_violation_rate_95 <- parametric_breaches_95 / total_days
parametric_violation_rate_99 <- parametric_breaches_99 / total_days

cat("Comparison of Historical and Parametric VaR:\n")
cat(sprintf("95%% Historical VaR: Violation Rate = %.2f%%\n", violation_rate_95 * 100))
cat(sprintf("95%% Parametric VaR: Violation Rate = %.2f%%\n", parametric_violation_rate_95 * 100))
cat(sprintf("99%% Historical VaR: Violation Rate = %.2f%%\n", violation_rate_99 * 100))
cat(sprintf("99%% Parametric VaR: Violation Rate = %.2f%%\n", parametric_violation_rate_99 * 100))
```

Parametric VaR for both 95% and 99% confidence levels are more realistic than the historical ones.
For 95%, the violation rate gets closer to the expected one but still under predicts the actual frequency of violations.
For 99%, it almost matches the expected 1%, meaning the violoation rate is almost perfect.
Historical VaR is more conservative, leaving no tail events to breach the threshold, while parametric VaR performs better especially at 99% confidence.

```{r}
breach_data <- data.frame(
  Confidence_Level = c("95%", "99%"),
  Historical_VaR = c(violation_rate_95, violation_rate_99) * 100,
  Parametric_VaR = c(parametric_violation_rate_95, parametric_violation_rate_99) * 100
)

ggplot(breach_data, aes(x = Confidence_Level)) +
  geom_bar(aes(y = Historical_VaR, fill = "Historical VaR"), stat = "identity", position = "dodge") +
  geom_bar(aes(y = Parametric_VaR, fill = "Parametric VaR"), stat = "identity", position = "dodge") +
  labs(title = "VaR Violation Rates", y = "Violation Rate (%)", x = "Confidence Level") +
  theme_minimal()


```

Parametric VaR provides more realistic thresholds at both confidence levels, historical VaR should be used for highly risk averse scenarios, where safer choisces are required.

## 4. Extreme Values Theory

### 4.1 Block Maxima

Weekly blocks were created by grouping the data into weeks based on the `Date` column.
For each week, the **maximum return** (largest loss) was calculated, representing the **worst weekly loss** due to the negative log transformation used in the return calculation.

```{r}
# Step 1: Prepare Weekly Block Maxima
raw_data$YearWeek <- format(raw_data$Date, "%Y-%U")  # Create Year-Week grouping

# Calculate block maxima (largest losses) for each week
block_maxima_weekly <- raw_data %>%
  group_by(YearWeek) %>%
  summarize(BlockMax = max(Return, na.rm = TRUE))  # Weekly maximum returns (largest losses)

# Step 2: Fit the GEV distribution
gev_fit_weekly <- fevd(block_maxima_weekly$BlockMax, type = "GEV")

# Step 3: Summarize GEV parameters
gev_summary <- summary(gev_fit_weekly)
print(gev_summary)


```

The table shows the **weekly block maxima** (largest losses) for the first six weeks in the dataset:

```{r}
  # Weekly periods
# Round the BlockMax values to 4 decimals
block_maxima_weekly$BlockMax <- round(block_maxima_weekly$BlockMax, 4)

# Select only the first 6 rows of the table
head_block_max <- head(block_maxima_weekly)

# Display the table (only the first 6 rows)
head_block_max


```

Each row highlights the **largest weekly loss** for the index.
For example, in the 49th week of 2020, the worst weekly loss was **2.7711**, which reflects a significant drop in returns during that week.

```{r}
# Step 4: Visualize Weekly Block Maxima
ggplot(block_maxima_weekly, aes(x = as.Date(paste0(YearWeek, "-1"), format = "%Y-%U-%u"), y = BlockMax)) +
  geom_line(color = "#27CED7") +
  geom_point(color = "#3E8853") +
  labs(
    title = "Weekly Block Maxima of Losses",
    x = "Week",
    y = "Block Maxima (Largest Losses)"
  ) +
  theme_minimal()

```

The graph visualizes the weekly **largest losses** over time, highlighting periods of heightened market stress, such as in 2022, where weekly losses reached their most extreme values.
Outside of 2022, the general trend indicates that most weekly losses are less severe, reflecting a relatively stable risk profile during those periods.
This visualization provides a clear timeline of extreme losses, offering insights into when and where the market experienced the most significant volatility.

#### 4.1.2 VaR and ES calculation

The table compares Value at Risk (VaR) and Expected Shortfall (ES) for 95% and 99% confidence levels using historical and parametric approaches.

```{r}
gev_params <- gev_fit_weekly$results$par
location <- gev_params["location"]  # Mu
scale <- gev_params["scale"]        # Sigma
shape <- gev_params["shape"]        # Xi

# Definir niveles de confianza
confidence_levels <- c(0.95, 0.99)

# Función para calcular VaR y ES usando la distribución GEV
gev_var_es <- function(location, scale, shape, confidence_level) {
  # Calcular VaR
  VaR <- location + (scale / shape) * ((-log(1 - confidence_level))^(-shape) - 1)
  
  # Calcular ES
  ES <- location + (scale / shape) * (
    (1 / (1 - shape)) * ((-log(1 - confidence_level))^(-shape)) - 1
  )
  
  return(list(VaR = -VaR, ES = -ES))  # Valores negativos para reflejar pérdidas
}

# Inicializar un data frame para almacenar los resultados
block_maxima_results <- data.frame(
  Confidence_Level = confidence_levels,
  Historical_VaR = numeric(length(confidence_levels)),
  Historical_ES = numeric(length(confidence_levels)),
  Parametric_VaR = numeric(length(confidence_levels)),
  Parametric_ES = numeric(length(confidence_levels))
)

# Calcular VaR y ES históricos y paramétricos para cada nivel de confianza
for (i in seq_along(confidence_levels)) {
  cl <- confidence_levels[i]
  
  # VaR histórico
  historical_var <- quantile(block_maxima_weekly$BlockMax, probs = 1 - cl, na.rm = TRUE)
  
  # ES histórico
  historical_es <- mean(block_maxima_weekly$BlockMax[block_maxima_weekly$BlockMax <= historical_var], na.rm = TRUE)
  
  # VaR y ES paramétricos usando la distribución GEV
  parametric_results <- gev_var_es(location, scale, shape, cl)
  parametric_var <- parametric_results$VaR
  parametric_es <- parametric_results$ES
  
  # Almacenar resultados
  block_maxima_results$Historical_VaR[i] <- historical_var
  block_maxima_results$Historical_ES[i] <- historical_es
  block_maxima_results$Parametric_VaR[i] <- parametric_var
  block_maxima_results$Parametric_ES[i] <- parametric_es
}


block_maxima_results %>%
  kable("html", caption = "Value at Risk and Expected Shortfall (Historical and Parametric)") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

The parametric approach shows a wider range, especially in ES at 95%, where the estimated loss magnitude is **significantly higher.** This discrepancy reflects the sensitivity of the parametric method, which assumes a heavier-tailed GEV model.
However, since the underlying distribution is known to be non-normal, the parametric estimates might overstate risks, particularly at lower confidence levels, emphasizing the need for careful model validation.

### 4.2 Peaks

By applying the Peaks Over Threshold (POT) method using the Open prices, we've estimated the Value at Risk (VaR) and Expected Shortfall (ES) for extreme events in the Nasdaq Composite Index.
This method provides an accurate and robust assessment of risk for extreme losses.

### 4.2.1. POT Parametric

**Threshold Selection**

The choice of threshold is crucial in the POT method.
An appropriate threshold balances bias and variance in parameter estimates.The mean excess plot helps identify a suitable threshold where the mean excess over the threshold is linear.
Based on the plot, a suitable threshold would be around -2 to 0, where the mean excess stabilizes and becomes linear.

```{r}
# Mean Excess Plot
meplot(-raw_data$Return, main = "Mean Excess Plot", col = "#42BA97")
```

The parameter stability plots provided a guidance to select a threshold.
The region 2.2 and 2.8 shows stability, making it reliable range for analyzing extreme events.

```{r}
# Define a sequence of thresholds
thresholds <- seq(quantile(-raw_data$Return, 0.90, na.rm = TRUE),
                  quantile(-raw_data$Return, 0.99, na.rm = TRUE),
                  length.out = 50)

# Initialize vectors to store parameters
xi_values <- numeric(length(thresholds))
beta_values <- numeric(length(thresholds))

# Fit the GPD for each threshold and store parameters
for (i in seq_along(thresholds)) {
  threshold <- thresholds[i]
  # Fit GPD only if threshold is not NA
  if (!is.na(threshold)) {
    fit <- gpd(-raw_data$Return, threshold = threshold)
    xi_values[i] <- fit$par.ests["xi"]
    beta_values[i] <- fit$par.ests["beta"]
  } else {
    xi_values[i] <- NA
    beta_values[i] <- NA
  }
}

# Create a data frame for plotting
gpd_params_df <- data.frame(
  Threshold = thresholds,
  Xi = xi_values,
  Beta = beta_values
)

# Plot parameter stability with custom colors
par(mfrow = c(1, 2))  # Arrange plots side by side

# Plot for Xi (Stability of Xi) with a custom color
plot(gpd_params_df$Threshold, gpd_params_df$Xi, type = "l",
     col = "#2683C6",  # Custom color for Xi
     xlab = "Threshold", ylab = "Xi",
     main = "Stability of Xi", lwd = 2)

# Plot for Beta (Stability of Beta) with a custom color
plot(gpd_params_df$Threshold, gpd_params_df$Beta, type = "l",
     col = "#42BA97",  # Custom color for Beta
     xlab = "Threshold", ylab = "Beta",
     main = "Stability of Beta", lwd = 2)

# Reset plotting parameters
par(mfrow = c(1, 1))

```

```{r}
# Step 1: Set the threshold based on stability analysis
u <- 2.5  # Chosen threshold from stability region

# Step 2: Extract exceedances over the threshold
excesses <- -raw_data$Return[raw_data$Return > u] - u

# Print the threshold and some of the exceedances for verification
print(paste("Selected Threshold (u):", u))
print(head(excesses))

```

Fitting the Generalized Pareto Distribution (GPD):

```{r}
# Fit the GPD to the exceedances
gpd_fit <- gpd(-raw_data$Return, threshold = u)

# Summarize the fitted model
kable(summary(gpd_fit))

```

Estimating Value at Risk (VaR) and Expected Shortfall (ES)

```{r}
# Estimated parameters
xi <- as.numeric(gpd_fit$par.ests["xi"])
beta <- as.numeric(gpd_fit$par.ests["beta"])

# Sample sizes
N <- length(raw_data$Return)
N_exc <- sum(-raw_data$Return > u)

# Confidence levels
p_levels <- c(0.99, 0.995, 0.999)

# Calculate VaR
VaR_POT <- sapply(p_levels, function(p) {
  VaR <- u + (beta / xi) * (((N_exc / (N * (1 - p)))^xi - 1))
  return(-VaR)  # Convert back to negative return
})

# Create a data frame for results
VaR_POT_df <- data.frame(
  Confidence_Level = p_levels,
  VaR = VaR_POT
)

# Display results
kable(print(VaR_POT_df))

```

```{r}
# Calculate ES
ES_POT <- sapply(1:length(p_levels), function(i) {
  p <- p_levels[i]
  VaR_p <- -VaR_POT[i]  # Use positive value for calculation
  ES <- (VaR_p / (1 - xi)) + ((beta - xi * u) / (1 - xi))
  return(-ES)  # Convert back to negative return
})

# Add ES to the data frame
VaR_POT_df$ES <- ES_POT

# Display results
kable(print(VaR_POT_df))

```

The analysis highlights the increasing severity of potential losses as confidence levels rise, with VaR and ES growing more extreme at 99.9% confidence.

**Stop-Loss Strategy Based on POT VaR**

```{r}
# Add VaR levels to the dataset
raw_data$VaR_POT_99 <- VaR_POT_df$VaR[VaR_POT_df$Confidence_Level == 0.99]

# Generate stop-loss signals
raw_data$Stop_Loss_POT_99 <- raw_data$Return <= raw_data$VaR_POT_99

# Count the number of triggers
num_triggers_POT_99 <- sum(raw_data$Stop_Loss_POT_99, na.rm = TRUE)
cat("Number of stop-loss activations with POT at 99%:", num_triggers_POT_99, "\n")

```

According to the next plot, which compares the cumulative returns of two strategies.
early (2021-2022) the performance of both strategies si similar, as extreme loss events are limited.But in late 2022, during a period of high market volatility, the stop-loss strategy reduce the significant losses, as it ca be observed between the two lines.

```{r}
# Cumulative returns without stop-loss
raw_data$Cumulative_Return_No_Stop <- cumsum(raw_data$Return)

# Cumulative returns with stop-loss POT
raw_data$Cumulative_Return_With_Stop_POT_99 <- cumsum(ifelse(raw_data$Stop_Loss_POT_99, 0, raw_data$Return))

# Plot cumulative returns
library(ggplot2)

# Create a data frame for cumulative returns
cumulative_returns <- data.frame(
  Date = rep(raw_data$Date, 2),
  Cumulative_Return = c(raw_data$Cumulative_Return_No_Stop,
                        raw_data$Cumulative_Return_With_Stop_POT_99),
  Strategy = rep(c("Without Stop-Loss", "With Stop-Loss (POT 99%)"),
                 each = nrow(raw_data))
)

# Custom colors for the strategies
strategy_colors <- c("Without Stop-Loss" = "#1CADE4",  # Light Blue
                     "With Stop-Loss (POT 99%)" = "#42BA97")  # Green

# Plot cumulative returns with custom colors
ggplot(cumulative_returns, aes(x = Date, y = Cumulative_Return, color = Strategy)) +
  geom_line(size = 0.8) +
  scale_color_manual(values = strategy_colors) +  # Apply custom colors
  labs(
    title = "Cumulative Returns with and without Stop-Loss Based on POT",
    x = "Date",
    y = "Cumulative Return (%)"
  ) +
  theme_minimal() +
  theme(
    legend.title = element_blank(),  # Remove legend title
    legend.position = "bottom"       # Position legend at the bottom
  )

```

The POT-based stop-loss strategy significantly outperforms the non-stop-loss approach in terms of long-term cumulative returns, highlighting its practicality for managing extreme risk.

### 4.2.2. POT Historic

```{r}
# Define the threshold (e.g., 95th percentile of negative returns)
threshold <- quantile(-raw_data$Return, 0.95, na.rm = TRUE)

# Print the selected threshold
cat("Selected Threshold:", threshold, "\n")

```

```{r}
# Extract excesses over the threshold
excesses <- -raw_data$Return[-raw_data$Return > threshold] - threshold

# Print the first few exceedances
cat("Excesses over the threshold:\n")
print(head(excesses))

```

Calculating the VaR historical with POT

```{r}
# Define confidence levels
confidence_levels <- c(0.99, 0.995, 0.999)

# Calculate Historical VaR using POT
historical_var_pot <- sapply(confidence_levels, function(cl) {
  quantile(excesses, probs = cl, na.rm = TRUE) + threshold  # Add back the threshold
})

# Print the Historical VaR results
historical_var_pot_df <- data.frame(
  Confidence_Level = paste0(confidence_levels * 100, "%"),
  Historical_VaR_POT = -historical_var_pot  # Convert back to negative returns
)

```

Calculating ES Historical with POT

```{r}
# Calculate Historical ES using POT
historical_es_pot <- sapply(1:length(confidence_levels), function(i) {
  var_threshold <- historical_var_pot[i] - threshold  # Find the excess threshold
  mean(excesses[excesses >= var_threshold], na.rm = TRUE) + threshold  # Add back the threshold
})

# Add ES to the DataFrame
historical_var_pot_df$Historical_ES_POT <- -historical_es_pot  # Convert back to negative returns

```

The table presents the **Historical VaR** and **Historical ES** calculated using the Peaks Over Threshold (POT) method at different confidence levels (99%, 99.5%, and 99.9%).
The **VaR values** increase as the confidence level rises, reflecting higher potential losses as the analysis focuses on more extreme events.

The **ES value** captures the average of these extreme losses, which is slightly worse than the 99.9% VaR.
These results provide a conservative and realistic estimate of potential losses, making the Historical POT approach useful for assessing extreme risk scenarios without relying on parametric assumptions.

```{r}
# Combine results into a DataFrame
historical_var_pot_df <- data.frame(
  Confidence_Level = paste0(confidence_levels * 100, "%"),
  Historical_VaR_POT = -historical_var_pot,  # Convert back to negative returns
  Historical_ES_POT = -historical_es_pot    # Convert back to negative returns
)

# Print the final results
print("Historical VaR and ES using POT:")
print(historical_var_pot_df)
```

The VaR grows as the confidence level increases, which is expected because higher confidence levels capture rarer, more extreme events.
The 99% VaR indicates that losses will not exceed 6.37% on 99 out of 100 trading days, while the ES shows that when losses exceed the VaR, the average loss is 6.88%.
As confidence levels increase, the VaR captures progressively worse losses (up to 6.83% at 99.9% confidence), showing the potential for larger losses in rarer scenarios.

### Models Comparison

1.  Historical VaR: estimates the worst case loss based on historical returns, it is simple and reflects historical market behavior, however it assumes the future will behave as the past and does not consider the extreme tail behavior beyond the historical range.
2.  Parametric VaR: uses statistical assumptions to estimate the worst case loss, instead of using historical data. It is useful for stable markets with normally distributed returns, however the assumption for normality may underestimate tail risks.
3.  Expected Shortfall: estimates the average loss in the worst case scenario, when losses exceed the VaR threshold. It considers tail risks beyond VaR, so relies on the same data or assumptions as the VaR model (same issues). It can also be overly conservative.
4.  Block Maxima Model: groups data into fixed time intervals and extracts the largest loss in each block. It is useful because it focuses on extreme events without considering a specific threshold. However, it only extracts one value per block, other extreme values are ignored.
5.  Peaks Over Threshold Method: focuses only on extreme losses that exceed a certain threshold. It provides more accurate estimates of tail risks for extreme events and generates VaR and ES for very high confiedence levels. However, selecting a threshold requires a lot of attentions.

Previously, we determined that parametric VaR was the less conservative and hence the more realistic, especially for a 99% confidence level.
POT is more flexible and can accurately predict both frequency and severity of extreme losses, specially for rare events.
While Block Maxima focuses only on the maximum loss in each block and discards smaller tail losses.
In order to determine which one is better between Block Maxima and POT:

```{r}

return_periods <- c(1, 2, 5, 10, 20, 50, 100)

gev_fit <- fevd(block_maxima_weekly$BlockMax, type = "GEV")

gev_params <- gev_fit$results$par
mu <- gev_params["location"]
sigma <- gev_params["scale"]
xi <- gev_params["shape"]

gev_return_levels <- sapply(return_periods, function(T) {
  mu - (sigma / xi) * (1 - (-log(1 - 1/T))^(-xi))
})

u <- 2.5
gpd_fit <- gpd(-raw_data$Return, threshold = u)

xi_pot <- gpd_fit$par.ests["xi"]
beta_pot <- gpd_fit$par.ests["beta"]

pot_return_levels <- sapply(return_periods, function(T) {
  u + (beta_pot / xi_pot) * ((T * sum(raw_data$Return > u) / length(raw_data$Return))^xi_pot - 1)
})

comparison_df <- data.frame(
  ReturnPeriod = return_periods,
  BlockMaxima = gev_return_levels,
  POT = pot_return_levels
)

print(comparison_df)

ggplot(comparison_df, aes(x = ReturnPeriod)) +
  geom_line(aes(y = BlockMaxima, color = "Block Maxima"), size = 1) +
  geom_line(aes(y = POT, color = "POT"), size = 1) +
  labs(
    title = "Return Levels Comparison: Block Maxima vs POT",
    x = "Return Period (Years)",
    y = "Return Level"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("Block Maxima" = "#2683C6", "POT" = "#42BA97"))


```

Looking at the table, Block Maxima model predicts higher return levels than the POT model, especially for extreme periods like 50 and beyond.
POT predicts more moderate estimates of return levels.
This is also shown in the graph, where POT has a curve which grows less sharply than Block Maxima, which suggests more stable and gradual increase in return levels.
In this sense, Block Maxima overestimates extreme values due to the few extreme events it considers for the model, so POT provides more stable estimates, which makes it a more reliable model for financial risk measurements.

Conclusion POT\>Block Maxima\> Parametric VaR, in terms of capturing frequency and severity and balancing risk estimation and conservatism.

### Recommendations:

1.  Concerning the Parametric VaR, the model is easy to compute and understands and works well with large data sets, however it assumes normality of returns which may not be realistic.This model is recommended for stable markets where the return distribution is normal.
    It can also be be complemented with ES.
    The expected shortfall model should be used with careful attention given that it is overly conservative and could lead to mistakes in the risk management decisions.

2.  Peaks over Threshold captures a larger range of extreme values because it uses the GDP and uses ALL exceedances above threshold.
    However, we recommend careful selection of the threshold to avoid bias results.
    This model is the most accurate for extreme events, so in order to have a good threshold selection we recommend the Mean Excess Plot as a diagnostic tool.

3.  Block Maxima is a useful model when data sets have periodic maxima or has extreme events at fixed periods.
    However, it only focuses on these values and ignores other possible data points.
    We recommend this model for climatology or natural disasters but not for assessing financial risks due to the volatility of this type of events.
