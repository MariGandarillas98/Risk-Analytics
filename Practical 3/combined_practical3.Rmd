---
title: "Practical 3"
output: html_document
date: "2024-11-19"
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Data Pre-processing

```{r, message =FALSE}
install.packages(c("extRemes", "lubridate", "dplyr", "ggplot2", "kableExtra"))

library("xts")
library("quantmod")
library(kableExtra)
library(ggplot2)
library(dplyr)
library(tseries)
library(PerformanceAnalytics)
library(extRemes) 
library(lubridate) 

```

```{r}
raw_data <- read.csv("INDEX_US_XNAS_COMP.csv")

# Prepare for time-series
raw_data$Date <- as.Date(raw_data$Date, format = "%m/%d/%Y")
raw_data$Open <- as.numeric(gsub(",", "", raw_data$Open))
raw_data <- raw_data[order(raw_data$Date), ]

# Computing the daily returns
raw_data$Return <- c(NA, diff(raw_data$Open) / head(raw_data$Open, -1) * 100)
raw_data <- raw_data[-1, ]

# Check if any missing values
print(paste0("NAs in column : ",colSums(is.na(raw_data))))

# Time-Series of Nasdaq
nasdaq_ts <- ts(raw_data$Open, 
              start = c(as.numeric(format(min(raw_data$Date), "%Y")), 
                        as.numeric(format(min(raw_data$Date), "%j"))),
              frequency = 365) # Daily data

open_xts <- xts(raw_data$Open, order.by = raw_data$Date)
```

#### 1.1 Plot the time-series

```{r}
plot(nasdaq_ts, main = "Open Prices Time Series", ylab = "Open Price", xlab = "Time", xaxt = "n")
axis(1, at = unique(floor(time(nasdaq_ts))))

print(summary(nasdaq_ts))
```

#### 1.2 Computing the daily returns

```{r}
# 1. Compute the daily returns
raw_data$Return <- c(NA, -diff(log(raw_data$Open)) * 100)  # Negative log returns in percentage
raw_data <- na.omit(raw_data)  # Remove NA values from the first row due to diff

# Plot daily returns
plot(raw_data$Date, raw_data$Return, type = "l", col = "#1CADE4", xlab = "Date", ylab = "Negative Log Returns (%)", main = "Negative Log Returns of Nasdaq")

```

The summary of the daily returns shows a wide range of values, with a minimum return of -6.84% and a maximum return of 7.13%, indicating significant volatility.
The mean return is 0.06%, suggesting a mild positive average return, while the median return is higher at 0.18%, indicating that most daily returns are slightly positive.

## 2. Value at Risk (VaR)

#### 2.1 Check for stationarity

```{r}
# 2. Check for stationarity using Augmented Dickey-Fuller test
adf_test <- adf.test(raw_data$Return)
print(adf_test)
if (adf_test$p.value > 0.05) {
  cat("The time series is non-stationary. Differencing will be performed to achieve stationarity.\n")
  raw_data$Stationary_Return <- diff(raw_data$Return)
  raw_data <- na.omit(raw_data)
}

```

The daily returns are stationary (pval \< 0.05),.

#### 2.2 Check normality

```{r}
# 3. Test for normality
shapiro_test <- shapiro.test(raw_data$Return)
print(shapiro_test)
if (shapiro_test$p.value > 0.05) {
  cat("The distribution appears to be normal. Proceeding with parametric VaR calculation.\n")
} else {
  cat("The distribution does not appear to be normal. Caution is advised when using parametric VaR methods.\n")
}

# Plot histogram and Q-Q plot to visually inspect normality
par(mfrow = c(1, 2))
hist(raw_data$Return, breaks = 30, main = "Histogram of Negative Log Returns", xlab = "Negative Log Returns (%)", col = "lightblue")
qqnorm(raw_data$Return, main = "Q-Q Plot of Negative Log Returns")
qqline(raw_data$Return, col = "red")
par(mfrow = c(1, 1))
```

The daily returns are not normally distributed.
Therefore, using parametric Value at Risk and Expected Shortfall could result in under/overestimating the risk.
We will therefore use the historical VaR and ES moving forward.

#### 2.3 Calculation of VaR

```{r}
# 4. Calculate Value at Risk (VaR)
confidence_levels <- c(0.95, 0.99)

# Historical VaR calculation
historical_var <- numeric(length(confidence_levels))
for (i in seq_along(confidence_levels)) {
  cl <- confidence_levels[i]
  historical_var[i] <- quantile(raw_data$Return, probs = 1 - cl)
}

# Output Historical VaR results
for (i in seq_along(confidence_levels)) {
  cat(sprintf("Historical VaR at %.0f%% confidence level: %.2f%%\n", 
              confidence_levels[i] * 100, historical_var[i]))
}
# Parametric VaR calculation
mu <- mean(raw_data$Return)  # Mean of returns
sigma <- sd(raw_data$Return) # Standard deviation of returns
confidence_levels <- c(0.95, 0.99)

# Calculate VaR for each confidence level
parametric_var <- numeric(length(confidence_levels))
for (i in seq_along(confidence_levels)) {
  z_alpha <- qnorm(1 - confidence_levels[i])  # Z value for the confidence level
  parametric_var[i] <- mu + z_alpha * sigma
}

# Output Parametric VaR results
for (i in seq_along(confidence_levels)) {
  cat(sprintf("Parametric VaR at %.0f%% confidence level: %.2f%%\n", 
              confidence_levels[i] * 100, parametric_var[i]))
}

```

We can interpret these results as: "With probability 5% (1%), the daily returns will be smaller than **-2.19% (-3.35%).**

T**he Historical VaR values indicate that, with 95% confidence, the worst expected daily loss is -2.46%, and with 99% confidence, it is -3.93%. In comparison, the Parametric VaR estimates the worst daily loss at -2.38% for the 95% confidence level and -3.35%** for the 99% confidence level.
While both methods suggest similar risk levels, the Historical VaR provides a more direct representation of past market behavior, whereas the Parametric VaR relies on statistical assumptions, offering a slightly more conservative estimate of potential losses.

```{r}


# Visualize VaR levels against daily returns
plot(raw_data$Date, raw_data$Return, type = "l", col = "#1CADE4", xlab = "Date", ylab = "Negative Log Returns (%)", main = "Negative Log Returns with VaR Levels")
abline(h = quantile(raw_data$Return, probs = 1 - 0.95), col = "#42BA97", lty = 2, lwd = 2)
abline(h = quantile(raw_data$Return, probs = 1 - 0.99), col = "#3E8853", lty = 2, lwd = 2)
legend("topright", legend = c("Negative Log Returns", "Historical VaR 95%", "Historical VaR 99%"), col = c("#1CADE4", "#42BA97", "#3E8853"), lty = c(1, 2, 2))

```

```{r}
# 5. Calculate Expected Shortfall (ES)
# Historical ES calculation
historical_es <- numeric(length(confidence_levels))
for (i in seq_along(confidence_levels)) {
  cl <- confidence_levels[i]
  historical_es[i] <- mean(raw_data$Return[raw_data$Return <= quantile(raw_data$Return, probs = 1 - cl)])
}

# Compare VaR and ES values
cat("\nComparison of VaR and ES:\n")
for (cl in confidence_levels) {
  h_var <- quantile(raw_data$Return, probs = 1 - cl)
  h_es <- mean(raw_data$Return[raw_data$Return <= historical_var])
  cat(sprintf("At %.0f%% confidence level:\n", cl * 100))
  cat(sprintf("  Historical VaR: %.2f%%, Historical ES: %.2f%%\n", h_var, h_es))
}
```

We can interpret these results as: "When the daily returns are lower than -2.19% (-3.35%), it is expected to be **-2.91**% (-4.29%).

```{r}
# 5. Combine VaR and ES into a data frame for comparison

levels <- data.frame(
  Confidence_Level = paste0(confidence_levels * 100, "%"),
  Historical_VaR = historical_var,
  Historical_ES = historical_es
)

# Compare VaR and ES values
cat("\nComparison of VaR and ES:\n")
for (i in seq_along(confidence_levels)) {
  cat(sprintf("At %.0f%% confidence level:\n", confidence_levels[i] * 100))
  cat(sprintf("  Historical VaR: %.2f%%, Historical ES: %.2f%%\n", historical_var[i], historical_es[i]))
}

historical_var <- levels[,1:2]
historical_es <- levels[,c(1,3)]
```

The Historical Expected Shortfall (ES) values a**re -3.39% at the 95% confidence level and -4.78% at the 99% confidence level, indicating the average loss that occurs beyond the VaR threshold. The Parametric Expected Shortfall (ES) values are 2.86% at the 95% confidence level and 3.71% at the 99% confidence level, which are positive, reflecting a different outcome. The parametric metho**d appears to indicate potential gains or less severe losses, as it assumes returns follow a normal distribution, which may not capture extreme losses as effectively as the historical approach.
This disparity highlights the limitations of the parametric model in accurately estimating the true risk of extreme events.

In conclusion, the Historical method provides a more conservative and realistic measure of risk, especially in tail events, by incorporating real past data, while the Parametric method, based on the assumption of normality, tends to underestimate extreme risk.

## 3. The Stop-Loss Strategy

### 3.1 Count the Triggers

```{r}
# Add VaR and ES thresholds to the data
raw_data$VaR_95 <- historical_var[1,2]
raw_data$VaR_99 <- historical_var[2,2]
raw_data$ES_95 <- historical_es[1,2]
raw_data$ES_99 <- historical_es[2,2]

# Identify when stop-loss is triggered
raw_data$Stop_Loss_95 <- raw_data$Return <= raw_data$VaR_95
raw_data$Stop_Loss_99 <- raw_data$Return <= raw_data$VaR_99
raw_data$Stop_Loss_ES_95 <- raw_data$Return <= raw_data$ES_95
raw_data$Stop_Loss_ES_99 <- raw_data$Return <= raw_data$ES_99

# Count triggers for each strategy
trigger_counts <- colSums(raw_data[, c("Stop_Loss_95", "Stop_Loss_99", "Stop_Loss_ES_95", "Stop_Loss_ES_99")])
names(trigger_counts) <- c("VaR 95%", "VaR 99%", "ES 95%", "ES 99%")

print("Number of Stop-Loss Activations:")
print(trigger_counts)

```

The stop-loss strategy was triggered a different number of times depending on the chosen risk measure (VaR or ES) and confidence level.
The 95% confidence level resulted in significantly more activations compared to the 99% confidence level, as expected.
This reflects the more conservative nature of the 99% threshold, which corresponds to rarer and more extreme market movements.
Expected Shortfall (ES) triggered fewer activations than VaR at the same confidence level, demonstrating its focus on the average of the extreme tail events rather than a specific quantile.

### 3.2 Cumulative Returns Analysis

```{r}
# Cumulative returns without stop-loss
raw_data$Cumulative_Return_No_Stop <- cumsum(raw_data$Return)

# Cumulative returns with stop-loss for each strategy
raw_data$Cumulative_Return_With_Stop_95 <- cumsum(ifelse(raw_data$Stop_Loss_95, 0, raw_data$Return))
raw_data$Cumulative_Return_With_Stop_99 <- cumsum(ifelse(raw_data$Stop_Loss_99, 0, raw_data$Return))
raw_data$Cumulative_Return_With_Stop_ES_95 <- cumsum(ifelse(raw_data$Stop_Loss_ES_95, 0, raw_data$Return))
raw_data$Cumulative_Return_With_Stop_ES_99 <- cumsum(ifelse(raw_data$Stop_Loss_ES_99, 0, raw_data$Return))

# Plot Cumulative returns without stop-loss and with stop-loss for each strategy
data.frame(
  Date = rep(raw_data$Date, 5),
  Cumulative_Return = c(
    raw_data$Cumulative_Return_No_Stop,
    raw_data$Cumulative_Return_With_Stop_95,
    raw_data$Cumulative_Return_With_Stop_99,
    raw_data$Cumulative_Return_With_Stop_ES_95,
    raw_data$Cumulative_Return_With_Stop_ES_99
  ),
  Strategy = rep(c("Without Stop-Loss", 
                   "With Stop-Loss (VaR 95%)", 
                   "With Stop-Loss (VaR 99%)", 
                   "With Stop-Loss (ES 95%)", 
                   "With Stop-Loss (ES 99%)"), 
                 each = nrow(raw_data))
) %>% ggplot( aes(x = Date, y = Cumulative_Return, color = Strategy)) +
  geom_line() +
  scale_color_manual(values = c(
    "Without Stop-Loss" = "blue", 
    "With Stop-Loss (VaR 95%)" = "red", 
    "With Stop-Loss (VaR 99%)" = "darkred", 
    "With Stop-Loss (ES 95%)" = "green", 
    "With Stop-Loss (ES 99%)" = "darkgreen"
  )) +
  labs(
    title = "Cumulative Returns with and without Stop-Loss Strategies",
    x = "Date",
    y = "Cumulative Return"
  ) +
  theme_minimal() +
  theme(legend.title = element_blank())

```

The analysis reveals a significant difference between the cumulative returns of strategies with and without stop-loss.
Specifically, the strategy with the stop-loss set at VaR 95% yielded the highest cumulative return, reaching 230, while the strategy without any stop-loss resulted in the lowest cumulative return at 57.
This suggests that implementing a VaR-based stop-loss strategy helps to mitigate large losses, leading to a more favorable overall performance.
The stop-loss strategy limits significant downturns by cutting losses early, which can be beneficial in volatile market conditions.
The stop-loss triggered during sharp declines in the portfolio, effectively preventing deeper drawdowns.
This is evident as the cumulative return with the stop-loss strategy is consistently higher, demonstrating the protective nature of the stop-loss in limiting the impact of negative returns.
However, one consideration when using a stop-loss is the potential for missing significant rebounds after a sharp downturn.
The stop-loss strategy may have exited positions at lower prices, missing the opportunity to capitalize on rebounds.

## 4. Extreme Values Theory

### 4.1 Block Minima

Weekly blocks were created by grouping the data into weeks based on the `Date` column.
For each week, the **minimum return (**most negative value) was calculated, representing the **worst weekly loss**.
This approach helps focus on extreme losses.

```{r}
# Step 1: Prepare Weekly Block Maxima
raw_data$YearWeek <- format(raw_data$Date, "%Y-%U")  # Create Year-Week grouping

# Calculate block maxima for each week
block_minima_weekly <- raw_data %>%
  group_by(YearWeek) %>%
  summarize(BlockMin = min(Return, na.rm = TRUE))  # Weekly minimum returns

# Step 2: Fit the GEV distribution
gev_fit_weekly <- fevd(block_minima_weekly$BlockMin, type = "GEV")

# Step 3: Summarize GEV parameters
gev_summary <- summary(gev_fit_weekly)



```

The table shows the **weekly block minima** (worst losses) for the first six weeks in the dataset:

```{r}
  # Weekly periods
# Round the BlockMax values to 4 decimals
block_minima_weekly$BlockMin <- round(block_minima_weekly$BlockMin, 4)

# Select only the first 6 rows of the table
head_block_min <- head(block_minima_weekly)

# Display the table (only the first 6 rows)
head_block_min


```

Each row highlights the **worst weekly performance** of the index.
For example, in the 45th week of 2020, the worst return was **-1.2435.**

```{r}
# Step 4: Visualize Weekly Block Maxima
ggplot(block_minima_weekly, aes(x = as.Date(paste0(YearWeek, "-1"), format = "%Y-%U-%u"), y = BlockMin)) +
  geom_line(color = "#27CED7") +
  geom_point(color = "#3E8853") +
  labs(
    title = "Weekly Block Minima of Losses",
    x = "Week",
    y = "Block Minima (Losses)"
  ) +
  theme_minimal()
```

The graph highlights the **weekly worst returns** (block minima) over time, showing periods of heightened market stress where losses were more severe, such as around 2022.
These spikes in negative returns indicate moments of increased volatility or external market pressures.
However, outside of 2022, the general trend shows that most weekly losses are not excessively large suggesting a more stable risk profile during those periods.

### 4.2 Peaks

By applying the Peaks Over Threshold (POT) method using the Open prices, we've estimated the Value at Risk (VaR) and Expected Shortfall (ES) for extreme events in the Nasdaq Composite Index.
This method provides an accurate and robust assessment of risk for extreme losses.

**Threshold Selection**

The choice of threshold is crucial in the POT method.
An appropriate threshold balances bias and variance in parameter estimates.The mean excess plot helps identify a suitable threshold where the mean excess over the threshold is linear.
Based on the plot, a suitable threshold would be around -2 to 0, where the mean excess stabilizes and becomes linear.

```{r}
# Mean Excess Plot
meplot(-raw_data$Return, main = "Mean Excess Plot", col = "#42BA97")
```

The parameter stability plots provided a guidance to select a threshold.
The region 2.2 and 2.8 shows stability, making it reliable range for analyzing extreme events.

```{r}
# Define a sequence of thresholds
thresholds <- seq(quantile(-raw_data$Return, 0.90, na.rm = TRUE),
                  quantile(-raw_data$Return, 0.99, na.rm = TRUE),
                  length.out = 50)

# Initialize vectors to store parameters
xi_values <- numeric(length(thresholds))
beta_values <- numeric(length(thresholds))

# Fit the GPD for each threshold and store parameters
for (i in seq_along(thresholds)) {
  threshold <- thresholds[i]
  # Fit GPD only if threshold is not NA
  if (!is.na(threshold)) {
    fit <- gpd(-raw_data$Return, threshold = threshold)
    xi_values[i] <- fit$par.ests["xi"]
    beta_values[i] <- fit$par.ests["beta"]
  } else {
    xi_values[i] <- NA
    beta_values[i] <- NA
  }
}

# Create a data frame for plotting
gpd_params_df <- data.frame(
  Threshold = thresholds,
  Xi = xi_values,
  Beta = beta_values
)

# Plot parameter stability with custom colors
par(mfrow = c(1, 2))  # Arrange plots side by side

# Plot for Xi (Stability of Xi) with a custom color
plot(gpd_params_df$Threshold, gpd_params_df$Xi, type = "l",
     col = "#2683C6",  # Custom color for Xi
     xlab = "Threshold", ylab = "Xi",
     main = "Stability of Xi", lwd = 2)

# Plot for Beta (Stability of Beta) with a custom color
plot(gpd_params_df$Threshold, gpd_params_df$Beta, type = "l",
     col = "#42BA97",  # Custom color for Beta
     xlab = "Threshold", ylab = "Beta",
     main = "Stability of Beta", lwd = 2)

# Reset plotting parameters
par(mfrow = c(1, 1))

```

```{r}
# Step 1: Set the threshold based on stability analysis
u <- 2.5  # Chosen threshold from stability region

# Step 2: Extract exceedances over the threshold
excesses <- -raw_data$Return[raw_data$Return > u] - u

# Print the threshold and some of the exceedances for verification
print(paste("Selected Threshold (u):", u))
print(head(excesses))

```

Fitting the Generalized Pareto Distribution (GPD):

```{r}
# Fit the GPD to the exceedances
gpd_fit <- gpd(-raw_data$Return, threshold = u)

# Summarize the fitted model
kable(summary(gpd_fit))

```

Estimating Value at Risk (VaR) and Expected Shortfall (ES)

```{r}
# Estimated parameters
xi <- as.numeric(gpd_fit$par.ests["xi"])
beta <- as.numeric(gpd_fit$par.ests["beta"])

# Sample sizes
N <- length(raw_data$Return)
N_exc <- sum(-raw_data$Return > u)

# Confidence levels
p_levels <- c(0.99, 0.995, 0.999)

# Calculate VaR
VaR_POT <- sapply(p_levels, function(p) {
  VaR <- u + (beta / xi) * (((N_exc / (N * (1 - p)))^xi - 1))
  return(-VaR)  # Convert back to negative return
})

# Create a data frame for results
VaR_POT_df <- data.frame(
  Confidence_Level = p_levels,
  VaR = VaR_POT
)

# Display results
kable(print(VaR_POT_df))

```

```{r}
# Calculate ES
ES_POT <- sapply(1:length(p_levels), function(i) {
  p <- p_levels[i]
  VaR_p <- -VaR_POT[i]  # Use positive value for calculation
  ES <- (VaR_p / (1 - xi)) + ((beta - xi * u) / (1 - xi))
  return(-ES)  # Convert back to negative return
})

# Add ES to the data frame
VaR_POT_df$ES <- ES_POT

# Display results
kable(print(VaR_POT_df))

```

The analysis highlights the increasing severity of potential losses as confidence levels rise, with VaR and ES growing more extreme at 99.9% confidence.

**Stop-Loss Strategy Based on POT VaR**

```{r}
# Add VaR levels to the dataset
raw_data$VaR_POT_99 <- VaR_POT_df$VaR[VaR_POT_df$Confidence_Level == 0.99]

# Generate stop-loss signals
raw_data$Stop_Loss_POT_99 <- raw_data$Return <= raw_data$VaR_POT_99

# Count the number of triggers
num_triggers_POT_99 <- sum(raw_data$Stop_Loss_POT_99, na.rm = TRUE)
cat("Number of stop-loss activations with POT at 99%:", num_triggers_POT_99, "\n")

```

According to the next plot, which compares the cumulative returns of two strategies.
early (2021-2022) the performance of both strategies si similar, as extreme loss events are limited.But in late 2022, during a period of high market volatility, the stop-loss strategy reduce the significant losses, as it ca be observed between the two lines.

```{r}
# Cumulative returns without stop-loss
raw_data$Cumulative_Return_No_Stop <- cumsum(raw_data$Return)

# Cumulative returns with stop-loss POT
raw_data$Cumulative_Return_With_Stop_POT_99 <- cumsum(ifelse(raw_data$Stop_Loss_POT_99, 0, raw_data$Return))

# Create a data frame for cumulative returns
cumulative_returns <- data.frame(
  Date = rep(raw_data$Date, 2),
  Cumulative_Return = c(raw_data$Cumulative_Return_No_Stop,
                        raw_data$Cumulative_Return_With_Stop_POT_99),
  Strategy = rep(c("Without Stop-Loss", "With Stop-Loss (POT 99%)"),
                 each = nrow(raw_data))
)

# Custom colors for the strategies
strategy_colors <- c("Without Stop-Loss" = "#1CADE4",  # Light Blue
                     "With Stop-Loss (POT 99%)" = "#42BA97")  # Green

# Plot cumulative returns with custom colors
ggplot(cumulative_returns, aes(x = Date, y = Cumulative_Return, color = Strategy)) +
  geom_line(size = 0.8) +
  scale_color_manual(values = strategy_colors) +  # Apply custom colors
  labs(
    title = "Cumulative Returns with and without Stop-Loss Based on POT",
    x = "Date",
    y = "Cumulative Return (%)"
  ) +
  theme_minimal() +
  theme(
    legend.title = element_blank(),  # Remove legend title
    legend.position = "bottom"       # Position legend at the bottom
  )

```

The POT-based stop-loss strategy significantly outperforms the non-stop-loss approach in terms of long-term cumulative returns, highlighting its practicality for managing extreme risk.
